{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a98c739c45025e75b7a8a39168517825",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do **not** write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select _File_ â†’ _Save and CheckPoint_)\n",
    "\n",
    "5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "2d6897acfb6f43e384c61fe94b3f74e0",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem 2. Text Classification.\n",
    "\n",
    "In this problem, we use the NLTK and scikit-learn libraries to perform text classificatoin tasks on the [NLTK Reuters corpus](http://www.nltk.org/book/ch02.html#reuters-corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "c7df5ce98c4947ef4f4cdfeee3aacdf5",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nose.tools import assert_equal, assert_is_instance\n",
    "from numpy.testing import assert_almost_equal, assert_array_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "03e2a909da3de3acf4392fc3f264c7f1",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The corpus contains 10,788 documents (`fileids`) which have been classified into 90 topics (categories). We will use those 10,788 documents to train a machine learning model, and try to predict which topic each document belongs to. So, we will first find categories for each element of fileids. Note that the categories in the Reuters corpus overlap with each other, simply because a news story often covers multiple topics. If a document has more than one category, we will use only the first category. Here's a function that extracts categories from the `fileids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "5855bc920f307908bff0646abdda3eed",
     "grade": false,
     "grade_id": "get_categories_from_fileids",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def get_categories_from_fileids(corpus, fileids):\n",
    "    \"\"\"\n",
    "    Finds categories for each element of 'fileids'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: An NLTK corpus.\n",
    "    fileids: A list of strings.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = [sorted(corpus.categories(fileids=f))[0] for f in fileids]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "5f5d56fb5d5923a377a00d678dbda9aa",
     "grade": false,
     "grade_id": "print_get_categories_from_fileids",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "categories = get_categories_from_fileids(reuters, reuters.fileids())\n",
    "print(categories[:5], '...', categories[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2f88831ac64147146c49d992ccf87449",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The Reuters data set has already been grouped into a training set and a test set. Here's a functoin that mimics `scikit-learn`'s `train_test_split()` function and organizes the text data and categories into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "a3f96aa7b3ad3bf814707fae885cb094",
     "grade": false,
     "grade_id": "train_test_split",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(corpus):\n",
    "    \"\"\"\n",
    "    Creates a training set and a test from the NLTK Reuters corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: An NLTK corpus.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A 4-tuple (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    train_fileids = [fileid for fileid in corpus.fileids() if fileid.startswith('train')]\n",
    "    X_train = [corpus.raw(fileids=fileid) for fileid in train_fileids]\n",
    "    y_train = get_categories_from_fileids(corpus, train_fileids)\n",
    "    \n",
    "    test_fileids = [fileid for fileid in corpus.fileids() if fileid.startswith('test')]\n",
    "    X_test = [corpus.raw(fileids=fileid) for fileid in test_fileids]\n",
    "    y_test = get_categories_from_fileids(corpus, test_fileids)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3794464cfa80b47c0ffaff096c27a265",
     "grade": false,
     "grade_id": "run_train_test_split",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reuters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b370bdce3c11c3a6a84aa1c94157f461",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We have 7,769 documents for training. Note that each data point in `X_train` (and `X_test`) is a text. To train a machine learning model on text data, we will use `CountVectorizer` to convert text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "372d7bc93fe8edace0939b180f853a7b",
     "grade": false,
     "grade_id": "print_X_train",
     "locked": true,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"There are {} training data.\".format(len(X_train)))\n",
    "print(\"An example of training data:\\n\\n{}\".format(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "87f2cacf66cd3e3802f569eaaad2258d",
     "grade": false,
     "grade_id": "markdown_4",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "And each training label is the cateogry that the corresponding document belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "e490a96fe2326103931dd6f54a465863",
     "grade": false,
     "grade_id": "print_y_train",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7f76d9295deab66aeae518c161b0e87b",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Classify documents using SVC and stop words.\n",
    "\n",
    "- Build a **pipeline** by using [Pipeline](http://scikit-learn.org/0.17/modules/generated/sklearn.pipeline.Pipeline.html),  [CountVectorizer](http://scikit-learn.org/0.17/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), and [LinearSVC](http://scikit-learn.org/0.17/modules/generated/sklearn.svm.LinearSVC.html). Name the first step `cv` (the `CountVectorizer` step) and the second step `svc` (the `LinearSVC` step).\n",
    "\n",
    "- Use English stop words. \n",
    "\n",
    "- Use default values for all parameters in `CountVectorizer()`. Use default values for all parameters in `LinearSVC()` execept for `random_state`.\n",
    "\n",
    "- Without the `random_state` parameter, the `LinearSVC` algorithm has a random element. If you provide an integer to the `random_state` paramter, the algorithm becomes determinitstic and reproducible. So, don't forget to set the `random_state` parameter in `LinearSVC()`.\n",
    "\n",
    "- It is not necessary that you use all of the other four arguments `(X_train, X_test, y_train, and y_test)`. You should decide which arguments are needed and which are not.\n",
    "\n",
    "- The function must return a tuple of a `Pipeline` instance and a numpy array of predicted values. So your function will look something like\n",
    "\n",
    "```python\n",
    "def classify_document(X_train, X_test, y_train, y_test, random_state):\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    clf = pipline.fit(...)\n",
    "    predicted = clf.predict(...)\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    return clf, predicted\n",
    "```\n",
    "\n",
    "- Refer to the [Introduction to Text Classification notebook](https://github.com/UI-DataScience/accy571-fa16/blob/master/Week9/notebooks/intro2tc.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "7384aea32fdf07b8a93c349bea1a41cd",
     "grade": false,
     "grade_id": "classify_document_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def classify_document(X_train, X_test, y_train, y_test, random_state):\n",
    "    \"\"\"\n",
    "    Creates a document term matrix and uses SVM classifier to make document classifications.\n",
    "    Uses English stop words.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: A list of strings.\n",
    "    y_train: A list of strings.\n",
    "    X_test: A list of strings.\n",
    "    random_state: A np.random.RandomState instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (clf, y_pred)\n",
    "    clf: A Pipeline instance.\n",
    "    y_pred: A numpy array.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return clf, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "5d202e51543a6989d7ae17e5ada60759",
     "grade": false,
     "grade_id": "run_classify_document",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "clf, y_pred = classify_document(X_train, X_test, y_train, y_test, random_state=0)\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print(\"SVC prediction accuracy = {0:3.1f}%\".format(100.0 * score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "101dc068dc3306313aa5e315c38a5894",
     "grade": true,
     "grade_id": "classify_document_test",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(clf, Pipeline)\n",
    "assert_is_instance(y_pred, np.ndarray)\n",
    "cv = clf.named_steps['cv']\n",
    "assert_is_instance(cv, CountVectorizer)\n",
    "assert_is_instance(clf.named_steps['svc'], LinearSVC)\n",
    "assert_equal(cv.stop_words, 'english')\n",
    "assert_equal(len(y_pred), len(y_test))\n",
    "assert_array_equal(y_pred[:5], ['trade', 'grain', 'crude', 'corn', 'palm-oil'])\n",
    "assert_array_equal(y_pred[-5:], ['acq', 'dlr', 'earn', 'ipi', 'gold'])\n",
    "assert_almost_equal(score, 0.87777409738323953)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

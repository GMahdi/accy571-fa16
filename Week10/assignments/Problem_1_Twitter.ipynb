{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a98c739c45025e75b7a8a39168517825",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do **not** write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select _File_ â†’ _Save and CheckPoint_)\n",
    "\n",
    "5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "96fb72f1dcda52a84268a7231d4366ee",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem 1. Twitter.\n",
    "\n",
    "In this problem, we will use the twitter API to extract a set of tweets, and perform a sentiment analysis on twitter data to classify tweets as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "6811defb28e16a211a301ded40078d25",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import tweepy as tw\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nose.tools import assert_equal, assert_is_instance, assert_true, assert_almost_equal\n",
    "from numpy.testing import assert_array_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e30d28d5898174309605b1514a1f461b",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Create a Twitter Application\n",
    "\n",
    "- Follow the [Introduction to Social Media, Twitter notebook](https://github.com/UI-DataScience/accy571-fa16/blob/master/Week10/notebooks/intro2smt.ipynb) to create a Twitter Application.\n",
    "\n",
    "- Save the credentials into a file at `/home/data_scientist/lessons/accy571_week10/twitter.cred`. Assuming you have already used the *Lessons* tab to download Week 10 notebooks, you can use the file manager on the Jupyter server dashboard to navigate to *lessons* $\\rightarrow$ *accy571_week10*, and click on `twitter.cred`. Clicking on a text file will open the file in a text editor. Alternatively, you can use a text editor such as `vim` in the terminal.\n",
    "\n",
    "- `twitter.cred` must have the following four credentials in order on separate lines:\n",
    "```\n",
    "Access Token\n",
    "Access Token Secret\n",
    "Consumer Key\n",
    "Consumer Secret\n",
    "```\n",
    "\n",
    "- Once you have stored your credientials, run the following code cells (you don't have to write any code in this section) to check if you are able to use the Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "79304f86ea6479e3c2e4d320d1560f66",
     "grade": false,
     "grade_id": "connect_twitter_api",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def connect_twitter_api(cred_file):\n",
    "    \n",
    "    # Order: Access Token, Access Token Secret, Consumer Key, Consumer SecretAccess\n",
    "    with open(cred_file) as fin:\n",
    "        tokens = [line.rstrip('\\n') for line in fin if not line.startswith('#')]\n",
    "\n",
    "    auth = tw.OAuthHandler(tokens[2], tokens[3])\n",
    "    auth.set_access_token(tokens[0], tokens[1])\n",
    "\n",
    "    return tw.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "bff516aefcbf8782549eed0f80fe0115",
     "grade": true,
     "grade_id": "connect_twitter_api_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Do NOT change file path or name of twitter.cred\n",
    "api = connect_twitter_api('/home/data_scientist/lessons/accy571_week10/twitter.cred')\n",
    "assert_equal(api.get_user('katyperry').screen_name, 'katyperry')\n",
    "assert_equal(api.get_user('justinbieber').created_at.strftime('%Y %m %d %H %M %S'), '2009 03 28 16 41 22')\n",
    "assert_equal(api.get_user('BarackObama').name, 'Barack Obama')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "50107cd06fec10cea0c40ae387c85686",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We will first train a model on the NLTK twitter corpus, and use it to classify a set of tweets fetched from the Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "d002abeed67f6272d912c396f745bd8b",
     "grade": false,
     "grade_id": "import_twitter_samples",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples as tws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b275bed72e45a3eaeccc85c69fb98f28",
     "grade": false,
     "grade_id": "markdown_4",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "`get_pos_neg_tweets()` in the following code cell creates a training set from the NLTK twitter corpus. Positive tweets are in `positive_tweets.json`, while negative tweets are in `negative_tweets.json`. The `data` and `targets` ararys are one-dimensional numpy arrays, where the first half are the positive tweets and the second half are the negative tweets. Every positive tweet is assigned a numerical label of 1 in `targets`, and negative tweets 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "003c7170e8f4aba887c75a7963980c30",
     "grade": false,
     "grade_id": "get_pos_neg_tweets_answer",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def get_pos_neg_tweets(corpus):\n",
    "    \"\"\"\n",
    "    Creates a training set from twitter_samples corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: The nltk.corpus.twitter_samples corpus.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (data, targets)\n",
    "    \"\"\"\n",
    "    \n",
    "    pos_tweets = np.array(tws.strings('positive_tweets.json'))\n",
    "    neg_tweets = np.array(tws.strings('negative_tweets.json'))\n",
    "\n",
    "    pos_labels = np.ones(pos_tweets.shape[0])\n",
    "    neg_labels = np.zeros(neg_tweets.shape[0])\n",
    "\n",
    "    targets = np.concatenate((pos_labels, neg_labels), axis=0)\n",
    "    data = np.concatenate((pos_tweets, neg_tweets), axis=0)\n",
    "    \n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "02ecfdced19d5c76230db1acedba0ba3",
     "grade": false,
     "grade_id": "print_data",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data, targets = get_pos_neg_tweets(tws)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "b1df45f31b850f46c544aec721a01dd9",
     "grade": false,
     "grade_id": "print_targets",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "96e586369719f0f058480c9952c35609",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We train on 80% of the data, and test the performance on the remaining 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "ef666bc1813e32205cd7bf7cbe8449ab",
     "grade": false,
     "grade_id": "train_test_split",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, targets, test_size=0.2, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fad55bb2bd74abfd1e073c0468a280bc",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Train a LinearSVC model.\n",
    "\n",
    "- Build a pipeline by using [Pipeline](http://scikit-learn.org/0.17/modules/generated/sklearn.pipeline.Pipeline.html), [TfidVectorizer](http://scikit-learn.org/0.17/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), and [LinearSVC](http://scikit-learn.org/0.17/modules/generated/sklearn.svm.LinearSVC.html). Name the first step `tf` (the TfidVectorizer step) and the second step `svc` (the LinearSVC step).\n",
    "\n",
    "- Use English stop words.\n",
    "- Use unigrams, bigrams, and trigrams.\n",
    "\n",
    "- Use default values for all parameters in `TfidVectorizer()`. Use default values for all parameters in `LinearSVC()` execept for `random_state`.\n",
    "\n",
    "- Without the random_state parameter, the `LinearSVC` algorithm has a random element. If you provide an integer to the random_state paramter, the algorithm becomes determinitstic and reproducible. So, don't forget to set the random_state parameter in `LinearSVC()`.\n",
    "\n",
    "- It is not necessary that you use all of the other four arguments `(X_train, X_test, y_train, and y_test)`. You should decide which arguments are needed and which are not.\n",
    "\n",
    "- The function must return a tuple of a `Pipeline` instance and a numpy array of predicted values. So your function will look something like\n",
    "\n",
    "```python\n",
    "def classify_document(X_train, X_test, y_train, y_test, random_state):\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    clf = Pipeline(...)\n",
    "    ### YOUR CODE HERE\n",
    "    y_pred = clf.predict(...)\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    return clf, y_pred\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "15bc9985911c11c81d82f72adc7e12f2",
     "grade": false,
     "grade_id": "train_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train(X_train, X_test, y_train, y_test, random_state):\n",
    "    \"\"\"\n",
    "    Creates a document term matrix and uses LinearSVC classifier to make document classifications.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: A list of strings.\n",
    "    y_train: A list of strings.\n",
    "    X_test: A list of strings.\n",
    "    random_state: A np.random.RandomState instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (clf, y_pred)\n",
    "    clf: A Pipeline instance.\n",
    "    y_pred: A numpy array.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return clf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "af5dfd32c26cee57fafc6f1bea4f8034",
     "grade": false,
     "grade_id": "train_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "clf, y_pred = train(X_train, X_test, y_train, y_test, random_state=check_random_state(0))\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print(\"SVC prediction accuracy = {0:5.1f}%\".format(100.0 * score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "79e3417f4bbe57e002b13e4a470867c3",
     "grade": true,
     "grade_id": "train_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(clf, Pipeline)\n",
    "assert_is_instance(y_pred, np.ndarray)\n",
    "tf = clf.named_steps['tf']\n",
    "assert_is_instance(tf, TfidfVectorizer)\n",
    "assert_is_instance(clf.named_steps['svc'], LinearSVC)\n",
    "assert_equal(tf.stop_words, 'english')\n",
    "assert_equal(tf.ngram_range, (1, 3))\n",
    "assert_equal(len(y_pred), len(y_test))\n",
    "assert_array_equal(y_pred[:10], [0, 1, 1, 0, 1, 0, 0, 0, 1, 1])\n",
    "assert_array_equal(y_pred[-10:], [0, 0, 1, 1, 1, 0, 0, 1, 1, 0])\n",
    "assert_almost_equal(score, 0.76400000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "79cce8b28b7474e470959fa42e46d668",
     "grade": false,
     "grade_id": "markdown_7",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "To apply our trained sentiment analysis pipeline on new twitter data, let's use Tweepy's [user_timeline()](http://docs.tweepy.org/en/latest/api.html#API.user_timeline) to extract 20 tweets from some users. Note that we specify the `max_id` parameter for reproducibility.\n",
    "\n",
    "(You don't have to write any code in the following code cells.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "91e9a6e560acc232a88e0c00bacac869",
     "grade": false,
     "grade_id": "get_timeline_answer",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def get_timeline(user, max_id):\n",
    "    \"\"\"\n",
    "    Fetches 20 tweets from 'user'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    user: A string. The ID or screen name of a Twitter user.\n",
    "    max_id: An int. Returns only statuses with an ID less than\n",
    "            (i.e., older than) or equal to the specified ID.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of integers.\n",
    "    \"\"\"\n",
    "    \n",
    "    timeline = api.user_timeline(id=user, count=20, max_id=max_id)\n",
    "    \n",
    "    return timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "4c554265ca41b72a0473168687a4868b",
     "grade": false,
     "grade_id": "get_timeline_run_1",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "timeline1 = get_timeline('HillaryClinton', max_id=790748347615371264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "4cd415e8d801a88c9a33ff01a29032fc",
     "grade": false,
     "grade_id": "get_timeline_run_2",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "timeline2 = get_timeline('realDonaldTrump', max_id=790730455129714688)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "50d374461d30ae757e51fc200cc549d2",
     "grade": false,
     "grade_id": "markdown_8",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Finally, we use the Linear SVC model to classify each tweet in the timelines as a positive tweet or a negative tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "3dd7a73da284e4a84a7478dc447ffeb5",
     "grade": false,
     "grade_id": "predict_answer",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def predict(clf, timeline):\n",
    "    \"\"\"\n",
    "    Uses a classifier (\"clf\") to classify each tweet in\n",
    "    \"timeline\" as a positive tweet or a negative tweet.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clf: A Pipeline instance.\n",
    "    timeline: A tweepy.models.ResultSet instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array.\n",
    "    \"\"\"\n",
    "    \n",
    "    texts = np.array([t.text for t in timeline])\n",
    "    y_pred = clf.predict(texts)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "dc3424163b17c51de2334787fb060a10",
     "grade": false,
     "grade_id": "predict_run_1",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "pred1 = predict(clf, timeline1)\n",
    "print('{} has {} positive tweets and {} negative tweets.'.format(\n",
    "    'Hillary Clinton', (pred1 == 1).sum(), (pred1 == 0).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "264cf842f06175390f0d4a5f45ce0e98",
     "grade": false,
     "grade_id": "predict_run_2",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "pred2 = predict(clf, timeline2)\n",
    "print('{} has {} positive tweets and {} negative tweets.'.format(\n",
    "    'Donald Trump', (pred2 == 1).sum(), (pred2 == 0).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

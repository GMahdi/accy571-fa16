{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Spark (DataFrames)\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Previously in this course, we have discussed doing data science at the\n",
    "Unix command line, and with Python, primarily by using Pandas. We also\n",
    "have discussed other Python libraries that bring new functionalities to\n",
    "the Python data science stack. Other _big data_ technologies, however,\n",
    "also exist and can be relevant to particular data science\n",
    "investigations, depending on the scale of data. Of these other\n",
    "technologies, one of the most promising is [**Spark**][sp].\n",
    "\n",
    "Spark is a cluster computing system that leverages [Hadoop][sh]\n",
    "technologies like [HDFS][shdfs] for high performance storage and\n",
    "[Yarn][sy] for cluster management. While some may see Spark as a\n",
    "replacement for Hadoop, an alternative argument can be made that [Spark\n",
    "is simply another compute engine][sce] for Hadoop, in addition to\n",
    "Map-Reduce.\n",
    "\n",
    "In this IPython Notebook, we explore using Spark to perform data\n",
    "processing in a similar maner to our previous efforts with Pandas. For\n",
    "this we will use the airline data, which has been stored in an HDFS\n",
    "system that is accesible from within our Spark cluster. [Other][dw]\n",
    "tutorials exist, although they often focus on Scala examples since Spark\n",
    "is written for that language.\n",
    "\n",
    "-----\n",
    "[sp]: http://spark.apache.org\n",
    "[sh]: http://hadoop.apache.org\n",
    "[sy]: https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html\n",
    "[shdfs]: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html\n",
    "[sce]: http://techcrunch.com/2015/07/12/spark-and-hadoop-are-friends-not-foes/\n",
    "[dw]: https://github.com/deanwampler/spark-workshop/tree/master/tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "In this class, we do not use a dedicated Spark cluster, and instead run\n",
    "our spark applications from within our\n",
    "JupyterHub Server environment. However, we still emphasize resource\n",
    "management, in particular we demonstrate how to ensure that any\n",
    "SparkContext previously used by this Jupyter Server is properly released\n",
    "before starting a new one. After this, we will initialize a new\n",
    "SparkContext to properly interact from this dockerized IPython Notebook\n",
    "to the Spark cluster.\n",
    "\n",
    "----- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Using Spark\n",
    "\n",
    "Spark is a framework for processing large-data tasks, in general this\n",
    "means Petabytes (or more of data). Spark can run on the HDFS file\n",
    "system, which can be set up to chunk files into blocks and to replicate\n",
    "these blocks across a cluster's storage to promote increased\n",
    "performance. Spark abstracts these details, however, allowing us to\n",
    "develop an application on a small system and scale up to large data on a\n",
    "cluster. \n",
    "\n",
    "In Spark, communications move between a driver process and the execution\n",
    "processes. This communication is handled for us by using a\n",
    "[`SparkContext`][sc], which requests resources from the Spark master\n",
    "process, such as number of cores, which are reserved to complete our\n",
    "Spark tasks. In the previous code cell, we initialized our\n",
    "`SparkContext`. Once a Spark Context is active, we can use the Spark\n",
    "Console to monitor jobs and the overall Spark infrastructure. The\n",
    "Jupyter Server currently sets an HTTP header (`X-Frame-Options`) that\n",
    "prevents us from easily displaying this console within this Notebook.\n",
    "However, if you open a new web browser to the IP address of this\n",
    "Notebook and use `4040` as the port number, you should be able to view\n",
    "and interact with the console, as shown in the following screenshot:\n",
    "\n",
    "![Spark Console](images/spark-console.png)\n",
    "\n",
    "-----\n",
    "\n",
    "The basic data structure in Spark is a [Resilient Distributed\n",
    "Dataset][rdd] (RDD). An RDD is immutable, thus if you want to add a\n",
    "column to an RDD, you must create a new copy that includes the new\n",
    "column. In Spark, data processing tasks can be transformation or\n",
    "actions, and these tasks can be pipelined for efficiency. Each\n",
    "transformation creates a new RDD, but since Spark uses lazy evaluation,\n",
    "the transformations are not executed until an action is invoked.\n",
    "\n",
    "These concepts are demonstrated in the following code cells, where we\n",
    "first create a list of integers, which we use to initialize a new RDD.\n",
    "\n",
    "\n",
    "-----\n",
    "[sc]: http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "[rdd]: http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Spark DataFrames \n",
    "\n",
    "In this IPython Notebook, we explore using Spark to perform data\n",
    "processing in a similar manner to our previous efforts with Pandas. For\n",
    "this we will use the airline data, which has been stored within a\n",
    "filesystem that is accessible from within our Spark cluster. We first\n",
    "initialize our spark environment, which in this Notebook is slightly\n",
    "different since we will connect our Spark environment to our Cassandra\n",
    "database. This requires additional Java libraries to be acquired and\n",
    "installed into the Spark environment, which will cause the Spark Context\n",
    "creation to take longer (so be patient).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup pySpark to be able to work with Cassandra\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \\\n",
    "    '--packages TargetHolding:pyspark-cassandra:0.3.5 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark version: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "# We release the SparkContext if it exists.\n",
    "try:\n",
    "    sc\n",
    "except:\n",
    "    pass ;\n",
    "else:\n",
    "    sc.stop()\n",
    "\n",
    "# Now handle initial import statements\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Create new Spark Configuration \n",
    "# Also set Cassandra host ip\n",
    "myconf = SparkConf()\n",
    "myconf.setMaster('local[*]')\n",
    "myconf.setAppName(\"ACCY 571: Professor Brunner\")\n",
    "myconf.set('spark.executor.memory', '1g')\n",
    "myconf.set('spark.cassandra.connection.host', '192.168.100.24')\n",
    "\n",
    "# Create and initialize a new Spark Context\n",
    "sc = SparkContext(conf=myconf)\n",
    "\n",
    "# Display Spark version information, which also verifies SparkContext is active\n",
    "print(\"\\nSpark version: {0}\".format(sc.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "In this Notebook, we will need sample data. To simplify acquiring data\n",
    "to demonstrate using Spark DataFrames, we include the RDD code from the\n",
    "[Introduction to Spark](intro2spark.ipynb) Notebook in the following\n",
    "cell.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in fields RDD = 480106\n"
     ]
    }
   ],
   "source": [
    "filename = '/home/data_scientist/data/2001/2001-1.csv'\n",
    "\n",
    "text_file = sc.textFile(filename)\n",
    "\n",
    "col_data = text_file.map(lambda l: l.split(\",\")) \\\n",
    "            .map(lambda p: (p[0], p[1], p[2], p[4], p[14], p[15], p[16], p[17], p[18])) \\\n",
    "            .filter(lambda line: 'Year' not in line)\n",
    "\n",
    "cols = col_data.filter(lambda line: 'NA' not in line)\n",
    "\n",
    "fields = cols.map(lambda p: (int(p[0]), int(p[1]), int(p[2]), int(p[3]),\n",
    "                          int(p[4]), int(p[5]), p[6], p[7], int(p[8])))\n",
    "\n",
    "# Should be 480106 if everything works correctly\n",
    "print('Number of entries in fields RDD = {0}'.format(fields.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Spark DataFrame\n",
    "\n",
    "Spark supports a simplified [Data Frame][spdf] as part of the [Spark\n",
    "SQL][spsql] library. We can create a Data Frame from an existing RDD by\n",
    "also specifying the column labels and data types. The data types must\n",
    "be one of the pre-defined [Spark SQL types][spdt]. After creating the\n",
    "new DataFrame (which is backed by an RDD), we can perform many of the\n",
    "same tasks with Spark that we performed with Pandas (but not all, and\n",
    "not in as simple of an approach). The following code cells show how we\n",
    "can take our 2001 flight data RDD and create a new Data Frame, which we\n",
    "subsequently use in several subsequent code cells.\n",
    "\n",
    "-----\n",
    "[spdf]: https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes\n",
    "[spsql]: https://spark.apache.org/sql/\n",
    "[spdt]: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# sc is an existing SparkContext.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "schemaString = \"Year Month DayOfMonth DepTime ArrDelay DepDelay Origin Destination Distance\"\n",
    "\n",
    "fieldTypes = [IntegerType(), IntegerType(), IntegerType(), \\\n",
    "              IntegerType(), IntegerType(), IntegerType(), \\\n",
    "              StringType(), StringType(), IntegerType()]\n",
    "\n",
    "f_data = [StructField(field_name, field_type, True) \\\n",
    "          for field_name, field_type in zip(schemaString.split(), fieldTypes)]\n",
    "\n",
    "schema = StructType(f_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Year: int, Month: int, DayOfMonth: int, DepTime: int, ArrDelay: int, DepDelay: int, Origin: string, Destination: string, Distance: int]\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(fields, schema)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "In the following three code cells, we `show` the first few lines of the\n",
    "DataFrame, then use the `head` method, which displays more syntactic\n",
    "information for each row, and finally use the `describe` method, which\n",
    "doesn't execute until the `show` action is invoked. While the output is\n",
    "less visually attractive than the Pandas result, we still obtain the\n",
    "necessary information.\n",
    "\n",
    "After these code cells, we access the DataFrame schema, first by using\n",
    "the `printSchema` method to nicely output the schema, and next access a\n",
    "column directly, which we can now do since we have named our DataFrame\n",
    "columns.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-------+--------+--------+------+-----------+--------+\n",
      "|Year|Month|DayOfMonth|DepTime|ArrDelay|DepDelay|Origin|Destination|Distance|\n",
      "+----+-----+----------+-------+--------+--------+------+-----------+--------+\n",
      "|2001|    1|        17|   1806|      -3|      -4|   BWI|        CLT|     361|\n",
      "|2001|    1|        18|   1805|       4|      -5|   BWI|        CLT|     361|\n",
      "|2001|    1|        19|   1821|      23|      11|   BWI|        CLT|     361|\n",
      "|2001|    1|        20|   1807|      10|      -3|   BWI|        CLT|     361|\n",
      "|2001|    1|        21|   1810|      20|       0|   BWI|        CLT|     361|\n",
      "+----+-----+----------+-------+--------+--------+------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Year=2001, Month=1, DayOfMonth=17, DepTime=1806, ArrDelay=-3, DepDelay=-4, Origin='BWI', Destination='CLT', Distance=361),\n",
       " Row(Year=2001, Month=1, DayOfMonth=18, DepTime=1805, ArrDelay=4, DepDelay=-5, Origin='BWI', Destination='CLT', Distance=361),\n",
       " Row(Year=2001, Month=1, DayOfMonth=19, DepTime=1821, ArrDelay=23, DepDelay=11, Origin='BWI', Destination='CLT', Distance=361),\n",
       " Row(Year=2001, Month=1, DayOfMonth=20, DepTime=1807, ArrDelay=10, DepDelay=-3, Origin='BWI', Destination='CLT', Distance=361)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+-----------------+-----------------+-----------------+------------------+-----------------+\n",
      "|summary|                Year| Month|       DayOfMonth|          DepTime|         ArrDelay|          DepDelay|         Distance|\n",
      "+-------+--------------------+------+-----------------+-----------------+-----------------+------------------+-----------------+\n",
      "|  count|              480106|480106|           480106|           480106|           480106|            480106|           480106|\n",
      "|   mean|              2001.0|   1.0|16.01370530674476|1359.660206287778|6.382288494624103| 8.781523246949632|716.9933556339641|\n",
      "| stddev|1.136732532560936...|   0.0|8.936964382456553|487.2369594358406|31.04865060768924|27.966300686761794|568.6557196351681|\n",
      "|    min|                2001|     1|                1|                1|              -80|               -59|               21|\n",
      "|    max|                2001|     1|               31|             2400|             1688|              1692|             4962|\n",
      "+-------+--------------------+------+-----------------+-----------------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayOfMonth: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Destination: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'Year'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can extract data from the DataFrame by using similar techniques to\n",
    "what we used with Pandas. One difference is that we need to `filter` the\n",
    "DataFrame, as opposed to directly access rows. However, we can filter\n",
    "rows to extract flights that left O'Hare, and secondly those flights\n",
    "that left O'Hare more than two hours late. In the second case, we also\n",
    "tranform the output to `select` the _Destination_ column and a new\n",
    "column that is the _Distance_ in kilometers.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27455"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['Origin'] == 'ORD').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|Destination| (Distance * 1.6)|\n",
      "+-----------+-----------------+\n",
      "|        PHL|           1084.8|\n",
      "|        CLT|958.4000000000001|\n",
      "|        MEM|            785.6|\n",
      "|        MEM|            785.6|\n",
      "|        MEM|            785.6|\n",
      "|        STL|            412.8|\n",
      "|        STL|            412.8|\n",
      "|        PVD|           1358.4|\n",
      "|        LAX|           2792.0|\n",
      "|        LAX|           2792.0|\n",
      "+-----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Origin'] == 'ORD').filter(df['DepDelay'] > 120).select(df['Destination'], df['Distance'] * 1.6).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Spark SQL\n",
    "\n",
    "Given a Spark DataFrame, we can apply SQL statements directly against\n",
    "the DataFrame by registering the DataFrame as a Spark temporary SQL\n",
    "table. The following code cells demonstrates this, as we register our\n",
    "DataFrame as a `flights` table, and execute a SQL statement to select\n",
    "the same data we obtained from our previous DataFrame filter.Since the\n",
    "data are unordered, we have different results displayed via the `show`\n",
    "method.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|Destination|Distance|\n",
      "+-----------+--------+\n",
      "|        PHL|     678|\n",
      "|        CLT|     599|\n",
      "|        MEM|     491|\n",
      "|        MEM|     491|\n",
      "|        MEM|     491|\n",
      "|        STL|     258|\n",
      "|        STL|     258|\n",
      "|        PVD|     849|\n",
      "|        LAX|    1745|\n",
      "|        LAX|    1745|\n",
      "+-----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(fields, schema)\n",
    "\n",
    "df.registerTempTable(\"flights\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "sql_q = \"SELECT Destination, Distance FROM flights WHERE Origin = 'ORD' AND DepDelay > 120\"\n",
    "\n",
    "results = sqlContext.sql(sql_q)\n",
    "\n",
    "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
    "results.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Cassandra Query\n",
    "\n",
    "We now connect to a remote database from a Spark application. In this\n",
    "case, we will use our existing Cassandra instance running on Microsoft\n",
    "Azure. We have already initialized the Spark context to acquire and\n",
    "install the spark-cassandra connector in the first code cell int his\n",
    "Notebook and we specified the host ip address for our Cassandra database\n",
    "as part of the Spark Context configuration parameters in the second code\n",
    "cell. Our next step is to establish a connection to a Cassandra keyspace\n",
    "and read data from a table into a Spark RDD. \n",
    "\n",
    "This last step is performed in the following cell. We first tell Spark\n",
    "to use the spark-cassandra driver, which will run in the Spark JVM, to\n",
    "connect to the database. Next, we load the `airlines` table from the\n",
    "`bigdog` keyspace in our Cassandra database. This table was created by\n",
    "using the following CQL query:\n",
    "\n",
    "```cql\n",
    "drop_schema = '''\n",
    "DROP TABLE IF EXISTS Airlines ;\n",
    "'''\n",
    "\n",
    "create_schema = '''\n",
    "CREATE TABLE Airlines (\n",
    "    Year int,\n",
    "    Month int,\n",
    "    DayOfMonth int,\n",
    "    DepTime int,\n",
    "    ArrDelay int,\n",
    "    DepDelay int,\n",
    "    Origin text,\n",
    "    Destination text,\n",
    "    Distance int,\n",
    "    PRIMARY KEY(Month, DayOfMonth, DepTime, Origin)\n",
    ");\n",
    "'''\n",
    "```\n",
    "\n",
    "The 2001 flight data we have analyzed previously in this Notebook has\n",
    "already been loaded into this table by using the following Python code:\n",
    "\n",
    "```python\n",
    "df.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "    options(table='airlines', keyspace='bigdog').save(mode=\"overwrite\")\n",
    "```\n",
    "\n",
    "One change from the previous Spark DataFrame used in this Notebook is\n",
    "the creation of the Column names (since CQL is case insensitive, while\n",
    "Spark is case sensitive), for the creation of this table, df was created\n",
    "with the following column names:\n",
    "\n",
    "```python\n",
    "schemaString = \"year month dayofmonth deptime arrdelay depdelay origin destination distance\"\n",
    "```\n",
    "\n",
    "After we load the data into the new flights RDD, we display the first\n",
    "few rows, and the column datatypes.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o97.load.\n: java.lang.ClassNotFoundException: org.apache.spark.Logging was removed in Spark 2.0. Please check if your library is compatible with Spark 2.0\n\tat org.apache.spark.sql.execution.datasources.DataSource.lookupDataSource(DataSource.scala:159)\n\tat org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:149)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:122)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NoClassDefFoundError: org/apache/spark/Logging\n\tat java.lang.ClassLoader.defineClass1(Native Method)\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n\tat java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$5$$anonfun$apply$1.apply(DataSource.scala:132)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$5$$anonfun$apply$1.apply(DataSource.scala:132)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$5.apply(DataSource.scala:132)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$5.apply(DataSource.scala:132)\n\tat scala.util.Try.orElse(Try.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource.lookupDataSource(DataSource.scala:132)\n\t... 16 more\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.Logging\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\t... 34 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e7adff234b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"org.apache.spark.sql.cassandra\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m               \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bigdog\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"airlines\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o97.load.\n: java.lang.ClassNotFoundException: org.apache.spark.Logging was removed in Spark 2.0. Please check if your library is compatible with Spark 2.0\n\tat org.apache.spark.sql.execution.datasources.DataSource.lookupDataSource(DataSource.scala:159)\n\tat org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:149)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:122)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NoClassDefFoundError: org/apache/spark/Logging\n\tat java.lang.ClassLoader.defineClass1(Native Method)\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n\tat java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$5$$anonfun$apply$1.apply(DataSource.scala:132)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$5$$anonfun$apply$1.apply(DataSource.scala:132)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$5.apply(DataSource.scala:132)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$5.apply(DataSource.scala:132)\n\tat scala.util.Try.orElse(Try.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource.lookupDataSource(DataSource.scala:132)\n\t... 16 more\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.Logging\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\t... 34 more\n"
     ]
    }
   ],
   "source": [
    "flights = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "               load(keyspace=\"bigdog\", table=\"airlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flights.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Given this Spark RDD, we can now perform subsequent operations as\n",
    "demonstrated in the [Introduction to Spark][intro2spark.ipynb] Notebook.\n",
    "Below, we apply several filters to the RDD to generate a subset of the\n",
    "full data. In this case, we select long flights from the Baltimore\n",
    "Washington International airport. Another option would be to create a\n",
    "DataFrame from this RDD and use the Spark DataFrame techniques presented\n",
    "earlier in this Notebook.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bwi = flights.filter(flights.origin == 'BWI').filter(flights.distance > 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bwi.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we introduced Spark DataFrames and Spark SQL.\n",
    "Now that you have run the Notebook, go back and make the following\n",
    "changes to see how the results change.\n",
    "\n",
    "1. Change the DataFrame to include different columns from the flights\n",
    "data. You might review the original [airline data\n",
    "set](http://stat-computing.org/dataexpo/2009/) website to see the column\n",
    "descriptions.\n",
    "\n",
    "2. Use a SQL query on the `df` DataFrame to compute the mean distance\n",
    "between all flights from O'Hare to Los Angeles International Airport\n",
    "(LAX).\n",
    "\n",
    "4. Add an index column to this Spark DataFrame, which sequentially\n",
    "increases.\n",
    "\n",
    "Additional, more advanced problems:\n",
    "\n",
    "1. Turn the Cassandra SQL RDD obtained previously in this Notebook into\n",
    "a Spark DataFrame and output the results of the `describe` function on\n",
    "all numeric columns.\n",
    "\n",
    "2. Turn this Spark DataFrame into a Pandas DataFrame and make a\n",
    "regression plot of the arrival delay versus the departure delay by using\n",
    "Seaborn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Ending the Spark Session\n",
    "\n",
    "We must stop the `SparkContext` in order to release resources on the\n",
    "instructional cluster before existing this Notebook.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

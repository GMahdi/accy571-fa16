{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Spark (DataFrames)\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Previously in this course, we have discussed doing data science at the\n",
    "Unix command line, and with Python, primarily by using Pandas. We also\n",
    "have discussed other Python libraries that bring new functionalities to\n",
    "the Python data science stack. Other _big data_ technologies, however,\n",
    "also exist and can be relevant to particular data science\n",
    "investigations, depending on the scale of data. Of these other\n",
    "technologies, one of the most promising is [**Spark**][sp].\n",
    "\n",
    "Spark is a cluster computing system that leverages [Hadoop][sh]\n",
    "technologies like [HDFS][shdfs] for high performance storage and\n",
    "[Yarn][sy] for cluster management. While some may see Spark as a\n",
    "replacement for Hadoop, an alternative argument can be made that [Spark\n",
    "is simply another compute engine][sce] for Hadoop, in addition to\n",
    "Map-Reduce.\n",
    "\n",
    "In this IPython Notebook, we explore using Spark to perform data\n",
    "processing in a similar maner to our previous efforts with Pandas. For\n",
    "this we will use the airline data, which has been stored in an HDFS\n",
    "system that is accesible from within our Spark cluster. [Other][dw]\n",
    "tutorials exist, although they often focus on Scala examples since Spark\n",
    "is written for that language.\n",
    "\n",
    "-----\n",
    "[sp]: http://spark.apache.org\n",
    "[sh]: http://hadoop.apache.org\n",
    "[sy]: https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html\n",
    "[shdfs]: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html\n",
    "[sce]: http://techcrunch.com/2015/07/12/spark-and-hadoop-are-friends-not-foes/\n",
    "[dw]: https://github.com/deanwampler/spark-workshop/tree/master/tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "In this class, we do not use a dedicated Spark cluster, and instead run\n",
    "our spark applications from within our\n",
    "JupyterHub Server environment. However, we still emphasize resource\n",
    "management, in particular we demonstrate how to ensure that any\n",
    "SparkContext previously used by this Jupyter Server is properly released\n",
    "before starting a new one. After this, we will initialize a new\n",
    "SparkContext to properly interact from this dockerized IPython Notebook\n",
    "to the Spark cluster.\n",
    "\n",
    "----- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Using Spark\n",
    "\n",
    "Spark is a framework for processing large-data tasks, in general this\n",
    "means Petabytes (or more of data). Spark can run on the HDFS file\n",
    "system, which can be set up to chunk files into blocks and to replicate\n",
    "these blocks across a cluster's storage to promote increased\n",
    "performance. Spark abstracts these details, however, allowing us to\n",
    "develop an application on a small system and scale up to large data on a\n",
    "cluster. \n",
    "\n",
    "In Spark, communications move between a driver process and the execution\n",
    "processes. This communication is handled for us by using a\n",
    "[`SparkContext`][sc], which requests resources from the Spark master\n",
    "process, such as number of cores, which are reserved to complete our\n",
    "Spark tasks. In the previous code cell, we initialized our\n",
    "`SparkContext`. Once a Spark Context is active, we can use the Spark\n",
    "Console to monitor jobs and the overall Spark infrastructure. The\n",
    "Jupyter Server currently sets an HTTP header (`X-Frame-Options`) that\n",
    "prevents us from easily displaying this console within this Notebook.\n",
    "However, if you open a new web browser to the IP address of this\n",
    "Notebook and use `4040` as the port number, you should be able to view\n",
    "and interact with the console, as shown in the following screenshot:\n",
    "\n",
    "![Spark Console](images/spark-console.png)\n",
    "\n",
    "-----\n",
    "\n",
    "The basic data structure in Spark is a [Resilient Distributed\n",
    "Dataset][rdd] (RDD). An RDD is immutable, thus if you want to add a\n",
    "column to an RDD, you must create a new copy that includes the new\n",
    "column. In Spark, data processing tasks can be transformation or\n",
    "actions, and these tasks can be pipelined for efficiency. Each\n",
    "transformation creates a new RDD, but since Spark uses lazy evaluation,\n",
    "the transformations are not executed until an action is invoked.\n",
    "\n",
    "These concepts are demonstrated in the following code cells, where we\n",
    "first create a list of integers, which we use to initialize a new RDD.\n",
    "\n",
    "\n",
    "-----\n",
    "[sc]: http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "[rdd]: http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Spark DataFrames \n",
    "\n",
    "In this IPython Notebook, we explore using Spark to perform data\n",
    "processing in a similar manner to our previous efforts with Pandas. For\n",
    "this we will use the airline data, which has been stored within a\n",
    "filesystem that is accessible from within our Spark cluster. We first\n",
    "initialize our spark environment, which in this Notebook is slightly\n",
    "different since we will connect our Spark environment to our Cassandra\n",
    "database. This requires additional Java libraries to be acquired and\n",
    "installed into the Spark environment, which will cause the Spark Context\n",
    "creation to take longer (so be patient).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup pySpark to be able to work with Cassandra\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \\\n",
    "    '--packages TargetHolding:pyspark-cassandra:0.3.5 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark version: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "# We release the SparkContext if it exists.\n",
    "try:\n",
    "    sc\n",
    "except:\n",
    "    pass ;\n",
    "else:\n",
    "    sc.stop()\n",
    "\n",
    "# Now handle initial import statements\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Create new Spark Configuration \n",
    "# Also set Cassandra host ip\n",
    "myconf = SparkConf()\n",
    "myconf.setMaster('local[*]')\n",
    "myconf.setAppName(\"ACCY 571: Professor Brunner\")\n",
    "myconf.set('spark.executor.memory', '1g')\n",
    "myconf.set('spark.cassandra.connection.host', '192.168.100.24')\n",
    "\n",
    "# Create and initialize a new Spark Context\n",
    "sc = SparkContext(conf=myconf)\n",
    "\n",
    "# Display Spark version information, which also verifies SparkContext is active\n",
    "print(\"\\nSpark version: {0}\".format(sc.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "In this Notebook, we will need sample data. To simplify acquiring data\n",
    "to demonstrate using Spark DataFrames, we include the RDD code from the\n",
    "[Introduction to Spark](intro2spark.ipynb) Notebook in the following\n",
    "cell.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in fields RDD = 480106\n"
     ]
    }
   ],
   "source": [
    "filename = '/home/data_scientist/data/2001/2001-1.csv'\n",
    "\n",
    "text_file = sc.textFile(filename)\n",
    "\n",
    "col_data = text_file.map(lambda l: l.split(\",\")) \\\n",
    "            .map(lambda p: (p[0], p[1], p[2], p[4], p[14], p[15], p[16], p[17], p[18])) \\\n",
    "            .filter(lambda line: 'Year' not in line)\n",
    "\n",
    "cols = col_data.filter(lambda line: 'NA' not in line)\n",
    "\n",
    "fields = cols.map(lambda p: (int(p[0]), int(p[1]), int(p[2]), int(p[3]),\n",
    "                          int(p[4]), int(p[5]), p[6], p[7], int(p[8])))\n",
    "\n",
    "# Should be 480106 if everything works correctly\n",
    "print('Number of entries in fields RDD = {0}'.format(fields.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Spark DataFrame\n",
    "\n",
    "Spark supports a simplified [Data Frame][spdf] as part of the [Spark\n",
    "SQL][spsql] library. We can create a Data Frame from an existing RDD by\n",
    "also specifying the column labels and data types. The data types must\n",
    "be one of the pre-defined [Spark SQL types][spdt]. After creating the\n",
    "new DataFrame (which is backed by an RDD), we can perform many of the\n",
    "same tasks with Spark that we performed with Pandas (but not all, and\n",
    "not in as simple of an approach). The following code cells show how we\n",
    "can take our 2001 flight data RDD and create a new Data Frame, which we\n",
    "subsequently use in several subsequent code cells.\n",
    "\n",
    "-----\n",
    "[spdf]: https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes\n",
    "[spsql]: https://spark.apache.org/sql/\n",
    "[spdt]: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# sc is an existing SparkContext.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "schemaString = \"Year Month DayOfMonth DepTime ArrDelay DepDelay Origin Destination Distance\"\n",
    "\n",
    "fieldTypes = [IntegerType(), IntegerType(), IntegerType(), \\\n",
    "              IntegerType(), IntegerType(), IntegerType(), \\\n",
    "              StringType(), StringType(), IntegerType()]\n",
    "\n",
    "f_data = [StructField(field_name, field_type, True) \\\n",
    "          for field_name, field_type in zip(schemaString.split(), fieldTypes)]\n",
    "\n",
    "schema = StructType(f_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Year: int, Month: int, DayOfMonth: int, DepTime: int, ArrDelay: int, DepDelay: int, Origin: string, Destination: string, Distance: int]\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(fields, schema)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "In the following three code cells, we `show` the first few lines of the\n",
    "DataFrame, then use the `head` method, which displays more syntactic\n",
    "information for each row, and finally use the `describe` method, which\n",
    "doesn't execute until the `show` action is invoked. While the output is\n",
    "less visually attractive than the Pandas result, we still obtain the\n",
    "necessary information.\n",
    "\n",
    "After these code cells, we access the DataFrame schema, first by using\n",
    "the `printSchema` method to nicely output the schema, and next access a\n",
    "column directly, which we can now do since we have named our DataFrame\n",
    "columns.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-------+--------+--------+------+-----------+--------+\n",
      "|Year|Month|DayOfMonth|DepTime|ArrDelay|DepDelay|Origin|Destination|Distance|\n",
      "+----+-----+----------+-------+--------+--------+------+-----------+--------+\n",
      "|2001|    1|        17|   1806|      -3|      -4|   BWI|        CLT|     361|\n",
      "|2001|    1|        18|   1805|       4|      -5|   BWI|        CLT|     361|\n",
      "|2001|    1|        19|   1821|      23|      11|   BWI|        CLT|     361|\n",
      "|2001|    1|        20|   1807|      10|      -3|   BWI|        CLT|     361|\n",
      "|2001|    1|        21|   1810|      20|       0|   BWI|        CLT|     361|\n",
      "+----+-----+----------+-------+--------+--------+------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Year=2001, Month=1, DayOfMonth=17, DepTime=1806, ArrDelay=-3, DepDelay=-4, Origin='BWI', Destination='CLT', Distance=361),\n",
       " Row(Year=2001, Month=1, DayOfMonth=18, DepTime=1805, ArrDelay=4, DepDelay=-5, Origin='BWI', Destination='CLT', Distance=361),\n",
       " Row(Year=2001, Month=1, DayOfMonth=19, DepTime=1821, ArrDelay=23, DepDelay=11, Origin='BWI', Destination='CLT', Distance=361),\n",
       " Row(Year=2001, Month=1, DayOfMonth=20, DepTime=1807, ArrDelay=10, DepDelay=-3, Origin='BWI', Destination='CLT', Distance=361)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+-----------------+-----------------+-----------------+------------------+-----------------+\n",
      "|summary|                Year| Month|       DayOfMonth|          DepTime|         ArrDelay|          DepDelay|         Distance|\n",
      "+-------+--------------------+------+-----------------+-----------------+-----------------+------------------+-----------------+\n",
      "|  count|              480106|480106|           480106|           480106|           480106|            480106|           480106|\n",
      "|   mean|              2001.0|   1.0|16.01370530674476|1359.660206287778|6.382288494624103| 8.781523246949632|716.9933556339641|\n",
      "| stddev|1.136732532560936...|   0.0|8.936964382456553|487.2369594358406|31.04865060768924|27.966300686761794|568.6557196351681|\n",
      "|    min|                2001|     1|                1|                1|              -80|               -59|               21|\n",
      "|    max|                2001|     1|               31|             2400|             1688|              1692|             4962|\n",
      "+-------+--------------------+------+-----------------+-----------------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayOfMonth: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Destination: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'Year'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can extract data from the DataFrame by using similar techniques to\n",
    "what we used with Pandas. One difference is that we need to `filter` the\n",
    "DataFrame, as opposed to directly access rows. However, we can filter\n",
    "rows to extract flights that left O'Hare, and secondly those flights\n",
    "that left O'Hare more than two hours late. In the second case, we also\n",
    "tranform the output to `select` the _Destination_ column and a new\n",
    "column that is the _Distance_ in kilometers.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27455"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['Origin'] == 'ORD').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|Destination| (Distance * 1.6)|\n",
      "+-----------+-----------------+\n",
      "|        PHL|           1084.8|\n",
      "|        CLT|958.4000000000001|\n",
      "|        MEM|            785.6|\n",
      "|        MEM|            785.6|\n",
      "|        MEM|            785.6|\n",
      "|        STL|            412.8|\n",
      "|        STL|            412.8|\n",
      "|        PVD|           1358.4|\n",
      "|        LAX|           2792.0|\n",
      "|        LAX|           2792.0|\n",
      "+-----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Origin'] == 'ORD').filter(df['DepDelay'] > 120).select(df['Destination'], df['Distance'] * 1.6).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Spark SQL\n",
    "\n",
    "Given a Spark DataFrame, we can apply SQL statements directly against\n",
    "the DataFrame by registering the DataFrame as a Spark temporary SQL\n",
    "table. The following code cells demonstrates this, as we register our\n",
    "DataFrame as a `flights` table, and execute a SQL statement to select\n",
    "the same data we obtained from our previous DataFrame filter.Since the\n",
    "data are unordered, we have different results displayed via the `show`\n",
    "method.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|Destination|Distance|\n",
      "+-----------+--------+\n",
      "|        PHL|     678|\n",
      "|        CLT|     599|\n",
      "|        MEM|     491|\n",
      "|        MEM|     491|\n",
      "|        MEM|     491|\n",
      "|        STL|     258|\n",
      "|        STL|     258|\n",
      "|        PVD|     849|\n",
      "|        LAX|    1745|\n",
      "|        LAX|    1745|\n",
      "+-----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(fields, schema)\n",
    "\n",
    "df.registerTempTable(\"flights\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "sql_q = \"SELECT Destination, Distance FROM flights WHERE Origin = 'ORD' AND DepDelay > 120\"\n",
    "\n",
    "results = sqlContext.sql(sql_q)\n",
    "\n",
    "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
    "results.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Ending the Spark Session\n",
    "\n",
    "We must stop the `SparkContext` in order to release resources on the\n",
    "instructional cluster before existing this Notebook.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

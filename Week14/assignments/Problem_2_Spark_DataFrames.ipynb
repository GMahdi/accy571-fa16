{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a98c739c45025e75b7a8a39168517825",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do **not** write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select _File_ â†’ _Save and CheckPoint_)\n",
    "\n",
    "5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d86850358a7114aae2c1952c4c45b474",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem 2. Spark DataFrames\n",
    "\n",
    "In this problem, we will use the Spark DataFrame to perform basic data processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "b2917543a448891dda658beb911975ed",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "import pandas as pd\n",
    "\n",
    "from nose.tools import assert_equal, assert_is_instance\n",
    "from numpy.testing.utils import assert_array_almost_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e1f5217e4ddd480f583703de23206888",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We run Spark in [local mode](http://spark.apache.org/docs/latest/programming-guide.html#local-vs-cluster-modes) from within our Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "afe9bbf46d15b1dc9b62bdb9cd8e29aa",
     "grade": false,
     "grade_id": "context",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ccccf6c7a85da36aedb4ac1e356b775a",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We create a new RDD by reading in the data as a textfile. \n",
    "We use the same [Iris flower data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) that we used in [Week 12 Problem 2](https://github.com/UI-DataScience/accy571-fa16/blob/master/Week12/assignments/Problem_2_Hadoop_File_System.ipynb) and [Week 13 Problem 1](https://github.com/UI-DataScience/accy571-fa16/blob/master/Week13/assignments/Problem_1_MapReduce.ipynb).\n",
    "Recall that the `iris.csv` file has 5 columns: `SepalLength`, `SepalWidth`, `PetalLength`, `PetalWidth`, and `Name`. The first 4 columns are floating point values, and the final column is a string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "1645f608045fae27ca6025898884c36c",
     "grade": false,
     "grade_id": "head",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "!head /home/data_scientist/data/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "9fd07d13f5cc9deb42acd4d3520c46ff",
     "grade": false,
     "grade_id": "text_file",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "csv_path = \"/home/data_scientist/data/iris.csv\"\n",
    "text_file = sc.textFile(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b565f6291bd3998b659eac8f97962597",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In the following code cell, we create a Spark DataFrame from `text_file`. The return type of `create_iris_df()` is a Spark DataFrame.\n",
    "```python\n",
    ">>> iris_df = create_iris_df(text_file)\n",
    ">>> iris_df.show()\n",
    "```\n",
    "```\n",
    "+-----------+----------+-----------+----------+\n",
    "|SepalLength|SepalWidth|PetalLength|PetalWidth|\n",
    "+-----------+----------+-----------+----------+\n",
    "|        5.1|       3.5|        1.4|       0.2|\n",
    "|        4.9|       3.0|        1.4|       0.2|\n",
    "|        4.7|       3.2|        1.3|       0.2|\n",
    "|        4.6|       3.1|        1.5|       0.2|\n",
    "|        5.0|       3.6|        1.4|       0.2|\n",
    "|        5.4|       3.9|        1.7|       0.4|\n",
    "|        4.6|       3.4|        1.4|       0.3|\n",
    "|        5.0|       3.4|        1.5|       0.2|\n",
    "|        4.4|       2.9|        1.4|       0.2|\n",
    "|        4.9|       3.1|        1.5|       0.1|\n",
    "|        5.4|       3.7|        1.5|       0.2|\n",
    "|        4.8|       3.4|        1.6|       0.2|\n",
    "|        4.8|       3.0|        1.4|       0.1|\n",
    "|        4.3|       3.0|        1.1|       0.1|\n",
    "|        5.8|       4.0|        1.2|       0.2|\n",
    "|        5.7|       4.4|        1.5|       0.4|\n",
    "|        5.4|       3.9|        1.3|       0.4|\n",
    "|        5.1|       3.5|        1.4|       0.3|\n",
    "|        5.7|       3.8|        1.7|       0.3|\n",
    "|        5.1|       3.8|        1.5|       0.3|\n",
    "+-----------+----------+-----------+----------+\n",
    "only showing top 20 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "a1068686ad9792b85e3c8f8cc1122793",
     "grade": false,
     "grade_id": "create_iris_df",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def create_iris_df(rdd):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.sql.dataframe.DataFrame instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    fields = (\n",
    "        rdd\n",
    "        .map(lambda line: line.split(\",\"))\n",
    "        .map(lambda items: (items[0], items[1], items[2], items[3]))\n",
    "        .filter(lambda line: 'SepalLength' not in line)\n",
    "        .map(lambda p: (float(p[0]), float(p[1]), float(p[2]), float(p[3])))\n",
    "        )\n",
    "    sqlContext = SQLContext(sc)\n",
    "    schemaString = \"SepalLength,SepalWidth,PetalLength,PetalWidth,Name\"\n",
    "    fieldTypes = [FloatType(), FloatType(), FloatType(), FloatType()]\n",
    "    f_data = [\n",
    "        StructField(field_name, field_type, True)\n",
    "        for field_name, field_type\n",
    "        in zip(schemaString.split(','), fieldTypes)\n",
    "        ]\n",
    "    schema = StructType(f_data)\n",
    "    df = sqlContext.createDataFrame(fields, schema)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "1e14445f9828fab45cd3df4a77f4a5e1",
     "grade": false,
     "grade_id": "create_iris_df_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "iris_df = create_iris_df(text_file)\n",
    "iris_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "00d79421663da79b42dc1b6913158f83",
     "grade": false,
     "grade_id": "markdown_4",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Filter and select\n",
    "\n",
    "- In the following code cell, write a function named `filter_and_average` that\n",
    "  1. Extracts all rows where `SepalLength` is strictly greater than 7.0 and `PetalLength` is strictly greater than 2.0, and\n",
    "  2. Transforms this output to return a data frame with a new column that is the average of the `SepalWidth` and `PetalWidth`.\n",
    "  \n",
    "  In other words, calculate the average of `SepalWidth` and `PetalWidth` for all rows with `SepalLength` > 7.0 and `PetalLength` > 2.0.\n",
    "  \n",
    "  \n",
    "Hints:\n",
    "\n",
    "- When you run\n",
    "  ```python\n",
    "  >>> answer = filter_and_average(iris_df)\n",
    "  >>> answer.show()\n",
    "  ```\n",
    "  you should get something like\n",
    "  ```\n",
    "  +---------------------------------+\n",
    "  |((SepalWidth + PetalWidth) * 0.5)|\n",
    "  +---------------------------------+\n",
    "  |                2.549999952316284|\n",
    "  |                2.549999952316284|\n",
    "  |               2.3499999046325684|\n",
    "  |                3.049999952316284|\n",
    "  |                              3.0|\n",
    "  |               2.4499998092651367|\n",
    "  |               2.4000000953674316|\n",
    "  |                              2.5|\n",
    "  |                2.299999952316284|\n",
    "  |               2.3499999046325684|\n",
    "  |               2.9000000953674316|\n",
    "  |               2.6500000953674316|\n",
    "  +---------------------------------+\n",
    "  ```\n",
    "  (Note that the column name does not have to match this example. It may be `(SepalWidth + PetalWidth) / 2` for instance. Furthermore, the values do not have to match exactly as long as they are very close (e.g., 2.549999952316284 vs 2.55). They are both correct as long as they are approximately equal.)\n",
    "- There are 12 rows that satisfy the criteria, `SepalLength` > 7.0 and `PetalLength` > 2.0:\n",
    "  ```python\n",
    "  >>> df = pd.read_csv(csv_path)\n",
    "  >>> mask = (df[\"SepalLength\"] > 7.0) & (df[\"PetalLength\"] > 2.0)\n",
    "  >>> long_iris = df.loc[mask, [\"SepalWidth\", \"PetalWidth\"]].reset_index(drop=True)\n",
    "  >>> print(len(long_iris))\n",
    "  ```\n",
    "  ```\n",
    "  12\n",
    "  ```\n",
    "  Thus, we have 12 rows in the first hint.\n",
    "  The average of `SepalWidth` and `PetalWidth` in the resulting Pandas DataFrame is\n",
    "  ```python\n",
    "  >>> print(df_widths.mean(axis=1))\n",
    "  ```\n",
    "  ```\n",
    "  0     2.55\n",
    "  1     2.55\n",
    "  2     2.35\n",
    "  3     3.05\n",
    "  4     3.00\n",
    "  5     2.45\n",
    "  6     2.40\n",
    "  7     2.50\n",
    "  8     2.30\n",
    "  9     2.35\n",
    "  10    2.90\n",
    "  11    2.65\n",
    "  ```\n",
    "  (As mentioend previously, the numbers do not have match exactly. They are both correct as long as they are very close.)\n",
    "- Use a combination of `filter()` and `select()`.\n",
    "- The [Introduction to Spark DataFrame notebook](https://github.com/UI-DataScience/accy571-fa16/blob/master/Week14/notebooks/sparkdf.ipynb) has some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "10bf2a5397d9d1e9db3fc0edaf53a2ce",
     "grade": false,
     "grade_id": "filter_and_average_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_and_average(df):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "95b514aad9170fae9244c76079ef98dc",
     "grade": false,
     "grade_id": "filter_and_average_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "answer = filter_and_average(iris_df)\n",
    "answer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "75c6c9b14b284da3e69371b67ca9c9d3",
     "grade": true,
     "grade_id": "filter_and_average_test",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# we compare Spark dataframe to Pandas dataframe\n",
    "df_pd = pd.read_csv(csv_path)\n",
    "mask = (df_pd[\"SepalLength\"] > 7.0) & (df_pd[\"PetalLength\"] > 2.0)\n",
    "df_widths = df_pd.loc[mask, [\"SepalWidth\", \"PetalWidth\"]].reset_index(drop=True)\n",
    "df_widths[\"AverageWidth\"] = 0.5 * (df_widths[\"SepalWidth\"] + df_widths[\"PetalWidth\"])\n",
    "df_widths = df_widths.drop([\"SepalWidth\", \"PetalWidth\"], 1)\n",
    "assert_array_almost_equal(answer.toPandas().values[:, 0], df_widths.values[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d6d40c5f8a3ed727255214fafde00273",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "We must stop the SparkContext in order to release the spark resources before existing this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "451e014b5a043d4218d283931290d8e9",
     "grade": false,
     "grade_id": "stop",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

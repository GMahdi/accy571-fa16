{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a98c739c45025e75b7a8a39168517825",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do **not** write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select _File_ â†’ _Save and CheckPoint_)\n",
    "\n",
    "5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b68189b5d15e2a9e458f29b7389d5d9b",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem 1. MapReduce\n",
    "\n",
    "In this problem, we will use Hadoop Streaming to execute a MapReduce code written in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "f031162e2abc7fcf51f9dd275606ff76",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nose.tools import assert_equal, assert_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b8dc574105df645997be9b0e271e2a83",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We use the same `iris.csv` file we first encountered in [Week 12 Problem 2](https://github.com/UI-DataScience/accy571-fa16/blob/master/Week12/assignments/Problem_2_Hadoop_File_System.ipynb). This is the [iris flower data set](https://en.wikipedia.org/wiki/Iris_flower_data_set). This is the [Iris flower data set](https://en.wikipedia.org/wiki/Iris_flower_data_set). As shown in the following code cell, this CSV file has 5 columns: `SepalLength`, `SepalWidth`, `PetalLength`, `PetalWidth`, and `Name`. The first 4 columns are floating point values, and the remaning column is a string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "1645f608045fae27ca6025898884c36c",
     "grade": false,
     "grade_id": "head",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "!head /home/data_scientist/data/iris.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5520fe0cd370c2056e943571c3f698e6",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The goal of this problem is to use MapReduce to find the maximum value in the **`SepalLength`** column. **We ignore all other columns for simplicity.**\n",
    "\n",
    "## Write a Mapper script in Python.\n",
    "\n",
    "- Write a Python script that\n",
    "  - Reads data from standard input (`STDIN`),\n",
    "  - Skips the first line (The first line of `iris.csv` is the header that has the column titles.), and\n",
    "  - Outputs to standard output (`STDOUT`) the `SepalLength` column.\n",
    "  \n",
    "Hints:\n",
    "- We read data from standard input, so your code should probably have something like `with sys.stdin as fin:`. We also output the results to standard output, so it should also have something like `with sys.stdout as fout:`. So, your code will probably look something like this:\n",
    "  ```python\n",
    "  with sys.stdin as fin:\n",
    "      with sys.stdout as fout:\n",
    "          # the rest is pseduo code\n",
    "          for each line in fin:\n",
    "              if header:\n",
    "                  continue\n",
    "              else:\n",
    "                  strip line, split line => a, b, c, d, e\n",
    "                  write a to fout\n",
    "  ```\n",
    "- We need to ignore the first row. There are many ways to do this, but I think the easiest way is to\n",
    "  - Use `enumerate` when iterating though each line, e.g. `for i, row in enumerate(fin):`,\n",
    "  - Check if the number of iteration is 0,\n",
    "  - If the number of iteration is 0, use `continue` to skip to the next iteration in the loop, and\n",
    "  - If the number of iteration is greater than 0, split the line by commas and write to standard output.\n",
    "  \n",
    "- In the end,\n",
    "  ```pyhton\n",
    "  >>> ! ./mapper.py < /home/data_scientist/data/iris.csv\n",
    "  ```\n",
    "  should give\n",
    "  ```\n",
    "  5.1\n",
    "  4.9\n",
    "  4.7\n",
    "  4.6\n",
    "  5.0\n",
    "  5.4\n",
    "  4.6\n",
    "  5.0\n",
    "  4.4\n",
    "  4.9\n",
    "  ...\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "5462e16b875b4b848820898cc6083cb3",
     "grade": false,
     "grade_id": "mapper_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make the file executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d81c1f337330a97c1da6fb2cf691422d",
     "grade": false,
     "grade_id": "chmod_mapper",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "!chmod u+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "69076afefe631ee832fb5960963a7907",
     "grade": false,
     "grade_id": "mapper_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! ./mapper.py < /home/data_scientist/data/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "40316bef87c8145473a4d4d06c11ccc0",
     "grade": true,
     "grade_id": "mapper_test",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test0 = \"\"\"SepalLength,SepalWidth,PetalLength,PetalWidth,Name\n",
    "5.1,3.5,1.4,0.2,Iris-setosa\n",
    "4.9,3.0,1.4,0.2,Iris-setosa\n",
    "4.7,3.2,1.3,0.2,Iris-setosa\n",
    "4.6,3.1,1.5,0.2,Iris-setosa\n",
    "5.0,3.6,1.4,0.2,Iris-setosa\"\"\".strip()\n",
    "\n",
    "with open(\"test0.csv\", \"w\") as f:\n",
    "    f.write(test0)\n",
    "    \n",
    "test0_out = ! ./mapper.py < test0.csv\n",
    "\n",
    "for i, row in enumerate(test0_out):\n",
    "    answer_comma = test0.split(\"\\n\")[i + 1]\n",
    "    answer = answer_comma.split(\",\")[0]\n",
    "    assert_equal(row, answer)\n",
    "    \n",
    "test1 = \"\"\"SepalLength,SepalWidth,PetalLength,PetalWidth,Name\n",
    "5.4,3.9,1.7,0.4,Iris-setosa\n",
    "4.6,3.4,1.4,0.3,Iris-setosa\n",
    "5.0,3.4,1.5,0.2,Iris-setosa\n",
    "4.4,2.9,1.4,0.2,Iris-setosa\"\"\".strip()\n",
    "\n",
    "with open(\"test1.csv\", \"w\") as f:\n",
    "    f.write(test1)\n",
    "    \n",
    "test1_out = ! ./mapper.py < test1.csv\n",
    "\n",
    "for i, row in enumerate(test1_out):\n",
    "    answer_comma = test1.split(\"\\n\")[i + 1]\n",
    "    answer = answer_comma.split(\",\")[0]\n",
    "    assert_equal(row, answer)\n",
    "\n",
    "!rm test0.csv test1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6656a4d86290036db860d862463bc0e7",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Write a Reducer script in Python.\n",
    "\n",
    "- Write a Python script that\n",
    "  - Reads values from standard input (`STDIN`),\n",
    "  - Finds the maximum value, and\n",
    "  - Outputs to standard output (`STDOUT`) the maximum value.\n",
    "\n",
    "Hints:\n",
    "\n",
    "- We read data from standard input, so your code should probably have something like `with sys.stdin as fin:`. We also output the results to standard output, so it should also have something like `with sys.stdout as fout:`. So, your code will probably look something like this:\n",
    "  ```python\n",
    "  with sys.stdin as fin:\n",
    "      with sys.stdout as fout:\n",
    "          # the rest is pseudocode\n",
    "          for each line in fin:\n",
    "              strip line => x\n",
    "              max_x = compare_max(x, max_x)\n",
    "          # outside the for loop\n",
    "          write max_x to fout\n",
    "  ```\n",
    "- You may want to use the provided `compare_max()` function. This function takes two values, converts the first value to a float if necessary, compares them, and returns the larger value. So, if you iterate through each row and use this function to update the maximum value, you will have the overall maximum at the end of the loop. For example,\n",
    "  ```python\n",
    "  >>> x = ['1', '4', '2', '3', '5', '1']\n",
    "  >>> maximum = -1\n",
    "  >>> for i in x:\n",
    "  ...     maximum = compare_max(i, maximum)\n",
    "  ...     print(\"current maximum:\", maximum)\n",
    "  >>> print(\"overall maximum:\", maximum)\n",
    "  ```\n",
    "  ```\n",
    "  current maximum: 1.0\n",
    "  current maximum: 4.0\n",
    "  current maximum: 4.0\n",
    "  current maximum: 4.0\n",
    "  current maximum: 5.0\n",
    "  current maximum: 5.0\n",
    "  overall maximum: 5.0\n",
    "  ```\n",
    "- In the end, \n",
    "  ```python\n",
    "  >>> ! ./mapper.py < /home/data_scientist/data/iris.csv | ./reducer.py\n",
    "  ```\n",
    "should give\n",
    "  ```\n",
    "  7.9\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2cd49c6b7e06d2fd3185a4267bef78a7",
     "grade": false,
     "grade_id": "reducer_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "def compare_max(current_value, current_max):\n",
    "    \"\"\"\n",
    "    Compares two values and returns the larger of\n",
    "    current_value and current_max.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    current_value: str or float.\n",
    "    current_max: float.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # if current_value is a string,\n",
    "        # convert it to a float\n",
    "        current_value = float(current_value)\n",
    "    except ValueError:\n",
    "        # if current_value cannot be converted to a float\n",
    "        # return None and exit immediately\n",
    "        return\n",
    "    \n",
    "    # if current_value > current_max, then current_value\n",
    "    # should be the new max. Return current_value as the max value.\n",
    "    if current_value > current_max:\n",
    "        return current_value\n",
    "    # if current_value < current_max, then current_max\n",
    "    # is still the max. Return current_max as the max value.\n",
    "    else:\n",
    "        return current_max\n",
    "    \n",
    "max_sepal_length = -1.\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "66a18bfc7594cebdccd6f566046bc04b",
     "grade": false,
     "grade_id": "chmod_reducer",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "!chmod u+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "0335cebf56e4027b6a7b957e8056d47e",
     "grade": false,
     "grade_id": "reducer_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! ./mapper.py < /home/data_scientist/data/iris.csv | ./reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "735596150d665677263eda5515cbb44c",
     "grade": true,
     "grade_id": "reducer_test",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test0 = \"\"\"SepalLength,SepalWidth,PetalLength,PetalWidth,Name\n",
    "5.1,3.5,1.4,0.2,Iris-setosa\n",
    "4.9,3.0,1.4,0.2,Iris-setosa\n",
    "4.7,3.2,1.3,0.2,Iris-setosa\n",
    "4.6,3.1,1.5,0.2,Iris-setosa\n",
    "5.0,3.6,1.4,0.2,Iris-setosa\"\"\".strip()\n",
    "\n",
    "with open(\"test0.csv\", \"w\") as f:\n",
    "    f.write(test0)\n",
    "    \n",
    "test0_out = ! ./mapper.py < test0.csv | ./reducer.py\n",
    "\n",
    "assert_equal(test0_out, ['5.1'])    \n",
    "\n",
    "test1 = \"\"\"SepalLength,SepalWidth,PetalLength,PetalWidth,Name\n",
    "5.4,3.9,1.7,0.4,Iris-setosa\n",
    "4.6,3.4,1.4,0.3,Iris-setosa\n",
    "5.0,3.4,1.5,0.2,Iris-setosa\n",
    "4.4,2.9,1.4,0.2,Iris-setosa\"\"\".strip()\n",
    "\n",
    "with open(\"test1.csv\", \"w\") as f:\n",
    "    f.write(test1)\n",
    "    \n",
    "test1_out = ! ./mapper.py < test1.csv | ./reducer.py\n",
    "\n",
    "assert_equal(test1_out, ['5.4'])\n",
    "\n",
    "! rm test0.csv test1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our mapper and reducer, so we are now ready to run Hadoop streaming. Let's first do some cleaning up and start up the namenode and datanodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "276c1db78b6567283d68f51384b5c09b",
     "grade": false,
     "grade_id": "start_nodes",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! $HADOOP_PREFIX/sbin/stop-dfs.sh\n",
    "! $HADOOP_PREFIX/sbin/stop-yarn.sh\n",
    "! rm -rf /tmp/*\n",
    "! echo \"Y\" | $HADOOP_PREFIX/bin/hdfs namenode -format 2> /dev/null\n",
    "! $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\n",
    "! $HADOOP_PREFIX/sbin/start-dfs.sh\n",
    "! $HADOOP_PREFIX/sbin/start-yarn.sh\n",
    "! $HADOOP_PREFIX/bin/hdfs dfsadmin -safemode leave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to copy the `iris.csv` file in our local environment to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "44046831aeb56f4e924493660f2576da",
     "grade": false,
     "grade_id": "hdfs_put",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! $HADOOP_PREFIX/bin/hdfs dfs -mkdir -p wc/in\n",
    "! $HADOOP_PREFIX/bin/hdfs dfs -put /home/data_scientist/data/iris.csv wc/in/iris.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run `mapper.py` and `reducer.py` via Hadoop Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "9bc835a02d7c6f2178d78f72deb81e54",
     "grade": false,
     "grade_id": "run_streaming",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run Python code via Hadoop streaming\n",
    "!$HADOOP_PREFIX/bin/hadoop jar \\\n",
    "    $HADOOP_PREFIX/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -input wc/in \\\n",
    "    -output wc/out \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d767a921c10e606b5833b4a10b06ee71",
     "grade": false,
     "grade_id": "print_wc_out",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ls_wc_out = !$HADOOP_PREFIX/bin/hdfs dfs -ls wc/out\n",
    "print('\\n'.join(ls_wc_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "36f4d9f7b99108401621b812d27ba552",
     "grade": false,
     "grade_id": "test_wc_out",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true('wc/out/_SUCCESS' in ls_wc_out.s)\n",
    "assert_true('wc/out/part-00000' in ls_wc_out.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code cell, we display the results of this Hadoop Streaming task output. The output should match the Python only MapReduce approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "5e3f7cb58cce54b41ce5ad0d7e4948af",
     "grade": false,
     "grade_id": "print_part",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -cat wc/out/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done. Having the namenode and datanodes running in the background consumes quite a bit of memory. So we should shut down the nodes at the end of the notebook. Make sure you run the assertion tests in the final code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "9d7156fff8ef048f73cdf0f979eecf80",
     "grade": false,
     "grade_id": "stop_nodes",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! $HADOOP_PREFIX/bin/hdfs dfs -rm -r -f -skipTrash wc/out\n",
    "! $HADOOP_PREFIX/sbin/stop-dfs.sh\n",
    "! $HADOOP_PREFIX/sbin/stop-yarn.sh\n",
    "! rm mapper.py reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "a73f649e3d37ee3c5d049cf166df61db",
     "grade": false,
     "grade_id": "test_stop",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "check_dfs_stopped = !$HADOOP_PREFIX/sbin/stop-dfs.sh\n",
    "assert_true(\"no namenode to stop\" in check_dfs_stopped.s)\n",
    "assert_true(\"no datanode to stop\" in check_dfs_stopped.s)\n",
    "assert_true(\"no secondarynamenode to stop\" in check_dfs_stopped.s)\n",
    "\n",
    "check_yarn_stopped = !$HADOOP_PREFIX/sbin/stop-yarn.sh\n",
    "assert_true(\"no resourcemanager to stop\" in check_yarn_stopped.s)\n",
    "assert_true(\"no nodemanager to stop\" in check_yarn_stopped.s)\n",
    "assert_true(\"no proxyserver to stop\" in check_yarn_stopped.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

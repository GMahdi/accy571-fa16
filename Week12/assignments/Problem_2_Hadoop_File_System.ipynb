{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a98c739c45025e75b7a8a39168517825",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do **not** write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select _File_ → _Save and CheckPoint_)\n",
    "\n",
    "5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c8a507d036a55af4f6dc125b8541314e",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem 2. Hadoop File System\n",
    "\n",
    "In this problem, we will explore some basic components of the Hadoop Distributed File System (HDFS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "f031162e2abc7fcf51f9dd275606ff76",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nose.tools import assert_equal, assert_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "29cb9ce54a58b5c4b73f33ce8fe14ee8",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We will first set up the a local Hadoop environement. Let's first stop the [namenode and datanodes](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html#NameNode+and+DataNodes) in case the nodes are running in the background from a previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "0ebc67013723d7eafd80eb5371a17200",
     "grade": false,
     "grade_id": "stop",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! $HADOOP_PREFIX/sbin/stop-dfs.sh\n",
    "! $HADOOP_PREFIX/sbin/stop-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b493b77a58693bf85456e2420fc55e02",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "If there are any temporary files created during the previous Hadoop operation, we want to clean them up. You may see something that looks like an error:\n",
    "\n",
    "```\n",
    "rm: cannot remove ‘/tmp/hsperfdata_root’: Operation not permitted\n",
    "```\n",
    "\n",
    "It's not really an error, and it won't affect our result, so you can safely ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d6c4b22eb88a2b1b047ffd31c5f0dee7",
     "grade": false,
     "grade_id": "rm_tmp",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! rm -rf /tmp/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f447c3057c3c02357e41a41861581345",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We will simply [format the namenode](https://wiki.apache.org/hadoop/GettingStartedWithHadoop#Formatting_the_Namenode) and delete all files in our HDFS. Note that our HDFS is in an ephemeral Docker container, so all data will be lost anyway when the Docker container is shut down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "6bc496fe44115ecf621dc1df6913eb1e",
     "grade": false,
     "grade_id": "namenode_format",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! echo \"Y\" | $HADOOP_PREFIX/bin/hdfs namenode -format 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b030ab281fdf5fa678ebf2a97b590506",
     "grade": false,
     "grade_id": "markdown_4",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "After formatting the namenode, we restart the namenode and datanodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "6d4d3a88f4253983b71fde9c0b9facfe",
     "grade": false,
     "grade_id": "start",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\n",
    "! $HADOOP_PREFIX/sbin/start-dfs.sh\n",
    "! $HADOOP_PREFIX/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d72d2ec8bcfa318ab7386b0645bb8fb7",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Sometimes when the namenode is restarted, it enteres [Safe Mode](https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Safemode), not allowing any changes to the file system. We do want to make changes, so we manually leave Safe Mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "40a65ca220dd508ee115790c892dda21",
     "grade": false,
     "grade_id": "safemode_leave",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! $HADOOP_PREFIX/bin/hdfs dfsadmin -safemode leave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "eea75cf9a3e8dec290ff2d0c82c836df",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Create a new directory /user/data_scientist in HDFS.\n",
    "\n",
    "- In the following code cell, ceate a new directory in HDFS at `/user/data_scientist/wc/in`\n",
    "\n",
    "- As the [Introduction to Hadoop notebook](https://github.com/UI-DataScience/accy571-fa16/blob/master/Week12/notebooks/intro2hadoop.ipynb) explains, we must use the [HDFS file system interface](https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfs) to move around the HDFS file system. We use `$HADOOP_PREFIX/bin/hdfs` to do this. Furthermore, `$HADOOP_PREFIX/bin/hdfs` is a Unix command, and to execute a Unix command in Jupyter notebook we must the ! magic. Putting them together, you answer should start with `!$HADOOP_PREFIX/bin/hdfs`.\n",
    "\n",
    "- Running `!$HADOOP_PREFIX/bin/hdfs` by itself will list the available commands:\n",
    "\n",
    "```python\n",
    "!$HADOOP_PREFIX/bin/hdfs\n",
    "```\n",
    "\n",
    "```\n",
    "Usage: hdfs [--config confdir] [--loglevel loglevel] COMMAND\n",
    "       where COMMAND is one of:\n",
    "  dfs                  run a filesystem command on the file systems supported in Hadoop.\n",
    "  ...\n",
    "```\n",
    "\n",
    "    where I only showed the first line because we only need the `dfs` subcommand. The [`dfs` commands](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html) mirrors many of the traditional Unix file systems commands. The full listing can be obtained by entering `!$HADOOP_PREFIX/bin/hdfs dfs`:\n",
    "\n",
    "```python\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs\n",
    "```\n",
    "\n",
    "```\n",
    "Usage: hadoop fs [generic options]\n",
    "    ...\n",
    "\t[-mkdir [-p] <path> ...]\n",
    "    ...\n",
    "```\n",
    "\n",
    "    Here, I only shortened the output to show only the relevant option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "a80ee262318b0411133303193f4f9d83",
     "grade": false,
     "grade_id": "mkdir_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "9bd884b3311c8084ccf383d9701a6be3",
     "grade": false,
     "grade_id": "mkdir_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ls_wc = ! $HADOOP_PREFIX/bin/hdfs dfs -ls wc\n",
    "print('\\n'.join(ls_wc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "3e9895cc6c52857b33a4106d62c97b53",
     "grade": true,
     "grade_id": "mkdir_test",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true('wc/in' in ls_wc.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b96d6371587f72f04fa59af9993f986d",
     "grade": false,
     "grade_id": "markdown_7",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Copy /home/data_scientist/data/iris.csv to /wc/in/iris.csv\n",
    "\n",
    "- There's a file called `iris.csv` in the `data` directory of the **local host file system**.\n",
    "\n",
    "```python\n",
    "!ls -lah /home/data_scientist/data/iris.csv\n",
    "```\n",
    "\n",
    "```\n",
    "-rw-r--r-- 1 root root 4.5K Nov  7 16:18 /home/data_scientist/data/iris.csv\n",
    "```\n",
    "\n",
    "    In the following code cell, copy this `iris.csv` file into the `wc/in` directory in **HDFS**.\n",
    "\n",
    "- Run `!$HADOOP_PREFIX/bin/hdfs dfs` again to find which option we need to use.\n",
    "\n",
    "```python\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs\n",
    "```\n",
    "\n",
    "```\n",
    "Usage: hadoop fs [generic options]\n",
    "    ...\n",
    "\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "6ec56a3caac131d7733388a320cd43ad",
     "grade": false,
     "grade_id": "put_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2e714caf91885eb0d870b24a0a5eba1b",
     "grade": false,
     "grade_id": "put_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ls_wc_in = ! $HADOOP_PREFIX/bin/hdfs dfs -ls wc/in\n",
    "print('\\n'.join(ls_wc_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "d798e7b0450c8b3da182cf9d9e54fa1b",
     "grade": true,
     "grade_id": "put_test",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true('wc/in/iris.csv' in ls_wc_in.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "73d14ef8018c5340b2b2ce52719b7861",
     "grade": false,
     "grade_id": "markdown_8",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We are done. Having the namenode and datanodes running in the background consumes quite a bit of memory. So we should shut down the nodes at the end of the notebook. Make sure you run the assertion tests in the final code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2d6d31acfd0551746f2d534d0a752bb9",
     "grade": false,
     "grade_id": "end",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "!$HADOOP_PREFIX/sbin/stop-dfs.sh\n",
    "!$HADOOP_PREFIX/sbin/stop-yarn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "02a4c34914de68ba3ee0263c3bba3bdb",
     "grade": false,
     "grade_id": "end_test",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "check_dfs_stopped = !$HADOOP_PREFIX/sbin/stop-dfs.sh\n",
    "assert_true(\"no namenode to stop\" in check_dfs_stopped.s)\n",
    "assert_true(\"no datanode to stop\" in check_dfs_stopped.s)\n",
    "assert_true(\"no secondarynamenode to stop\" in check_dfs_stopped.s)\n",
    "\n",
    "check_yarn_stopped = !$HADOOP_PREFIX/sbin/stop-yarn.sh\n",
    "assert_true(\"no resourcemanager to stop\" in check_yarn_stopped.s)\n",
    "assert_true(\"no nodemanager to stop\" in check_yarn_stopped.s)\n",
    "assert_true(\"no proxyserver to stop\" in check_yarn_stopped.s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

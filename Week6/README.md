#Week 6: Linear Models and Supervised Learning #
### Objectives ###

In this week, you will build on previous material to develop more
powerful techniques for first computing general linear models by using Python before moving on to supervised learning.
First, you will learn how to extend regression concepts to categorical variables, such as a variable that encodes a _Yes/No_ result. Second, 
you will learn about regularization and
how it can reduce the likelihood that your model might result in
overfitting and how it can be used to select the best attributes for
quantifying a model. Next, we transition to several basic supervised learning algorithms, including the k-nearest neighbor algorithm, support vector machine, and naive Bayes. Each of these algorithms have Python
implementations in the scikit learn library and are easy to apply to a
wide range of classification or regression problems. In addition, you
will learn about classification and regression metrics, such as the
precision, recall, and f1 score, and how to use them effectively to
compare the results of different machine learning algorithms. Finally, you will learn about you will learn about decision trees and ensemble techniques that are often built on tree algorithms to produce more robust machine learning predictions. The two main ensemble approaches we will cover are bagging and boosting, which differ in how weak learners are created and combined. The random forest algorithm is an example of a bagging algorithm, which is extremely popular due to its flexibility and ease of use. 

#####By the end of this lesson, you should be able to:######

- Understand logistic regression and how to compute it by using Python.
- Understand the concept of regularization and be able to apply it to regression problems by using Python.
- Know the basic classification and regression metrics.
- Understand the k-nearest neighbor algorithm
- Understand the support vector machine algorithm
- Understand the naive Bayes algorithm
- Understand the decision tree algorithm
- Understand the basic concept behind ensemble techniques
- Know the difference between bagging and boosting.
- Understand the random forest algorithm

### Activities and Assignments ###

|Activities and Assignments | Time Estimate | Deadline* |
|:------| -----|----------:|
|**[Week 6 Lesson 1: Introduction to Logistic Regression](lesson1.md)**| 2 Hours |Monday|
|**[Week 6 Lesson 2: Introduction to Regularization](lesson2.md)**| 2 Hours | Monday |
|**[Week 6 Lesson 3: Introduction to Supervised Learning:](lesson3.md)**| 3 Hours | Wednesday|
|**[Week 6 Lesson 4: Introduction to Decision Trees and Ensemble Learning](lesson4.md)**| 3 Hours | Wednesday|

*Please note that unless otherwise noted, the due time is 6pm Central time!

----------

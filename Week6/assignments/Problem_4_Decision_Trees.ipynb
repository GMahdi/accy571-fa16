{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a98c739c45025e75b7a8a39168517825",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do **not** write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select _File_ â†’ _Save and CheckPoint_)\n",
    "\n",
    "5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "2c97551f772ab70e7ad7cd90a1ab570d",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem 4. Decision Trees.\n",
    "\n",
    "In this problem, we fit a decision tree classifier that takes the day of the week and depature delays as input and predicts whether a flight is on time or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "eea18a7d53a6e30bd33ccec29585ae2d",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nose.tools import assert_is_instance, assert_equal, assert_almost_equal\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal\n",
    "from pandas.util.testing import assert_index_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4db03a35fa6757ccdb2b9a0ad63f9229",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We use the same [airline on-time performance data](http://stat-computing.org/dataexpo/2009/) from the lessons. You can find the descriptions [here](http://stat-computing.org/dataexpo/2009/). We use five columns: `Month`, `DayOfWeek`, `ArrDelay` `DepDelay`, and `Origin`. (Note: we are using one more column than we did in problems 2 and 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "87c889c7e7b203c18174f86b2796c3a8",
     "grade": false,
     "grade_id": "read_csv",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "filename = \"/home/data_scientist/data/2001.csv\"\n",
    "usecols = (1, 3, 14, 15, 17)\n",
    "names = [\"Month\", \"DayOfWeek\", \"ArrDelay\", \"DepDelay\", \"Origin\"]\n",
    "\n",
    "all_data = pd.read_csv(filename, header=0, na_values=[\"NA\"], usecols=usecols, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e94c555db8fc2e1652b4dc9eec98a851",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We perform some data pre-processing, similarly to the [Introduction to Logistic Regression](https://github.com/UI-DataScience/accy571-fa16/blob/master/Week6/notebooks/intro2lr.ipynb) notebook.\n",
    "\n",
    "To simplify the computations, we first extract only those flights that depart from Willard airport (CMI). After this, we drop all rows that have missing values (\"`NA`\") in any of the columns.\n",
    "\n",
    "We next create a categorical column, _arrival late_, that is zero if the flight arrived less than 5 minutes after the scheduled arrival time, or one if it arrived more than this number of minutes after the scheduled time. We will use this\n",
    "to train our logistic regressor.\n",
    "\n",
    "To save memory, we drop the columns that we no longer need: the origin airport and arrival delay columns.\n",
    "\n",
    "Finally, we reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "96ebf139f599b06fe159f020862a101a",
     "grade": false,
     "grade_id": "local",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "local = all_data[all_data[\"Origin\"] == \"CMI\"].dropna()\n",
    "\n",
    "local[\"ArrLate\"] = (local[\"ArrDelay\"] > 5).astype(int)\n",
    "\n",
    "local = local.drop([\"Origin\", \"ArrDelay\"], axis=1)\n",
    "\n",
    "local = local.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "042e8b252758fbfad72357151bc4026b",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Let's print out the first 10 columns of the resulting data frame.\n",
    "\n",
    "```python\n",
    ">>> print(local.head(10))\n",
    "```\n",
    "```\n",
    "   Month  DayOfWeek  DepDelay  ArrLate\n",
    "0      1          1      15.0        1\n",
    "1      1          2      -5.0        1\n",
    "2      1          3      52.0        1\n",
    "3      1          4      12.0        0\n",
    "4      1          5       0.0        0\n",
    "5      1          7     152.0        1\n",
    "6      1          1      51.0        1\n",
    "7      1          2       3.0        0\n",
    "8      1          3      -7.0        0\n",
    "9      1          4      14.0        0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "10bc0bf383d16b5dfb942bbf5539daa3",
     "grade": false,
     "grade_id": "print_local",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(local.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "73069a56d2f7e35e87a512f7426b5ede",
     "grade": false,
     "grade_id": "markdown_4",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In the previous problems, we used a validation set to adjust the hyperparameters. The <a href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation\">k-fold cross-validation</a> technique extends the idea of validation set. You can read more about $k$-fold CV on <a href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation\">Wikipedia</a> or [scikit-learn docs](http://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "\n",
    "We are going to use $k$-fold CV to\n",
    "\n",
    "1. measure the performance increase with increasing number of trees, and\n",
    "2. optimize one of the hyperparameters, `max_features`.\n",
    "We split `local` into a training set (used for training our model), a validation set (used for determining hyperparameters), and a test set (used for evaluating our model's performance).\n",
    "\n",
    "In the following code cell, we first split `local` into 80:20 training and test sets, and then use [sklearn.cross_validation.KFold()](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html) to get a $k$-fold cross-validation iterator with `n_fold=4` (4-fold cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d7ade511448854588663806ad8c95d9b",
     "grade": false,
     "grade_id": "train_test_split",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model by splitting into train and test sets\n",
    "X = local.drop(\"ArrLate\", axis=1)\n",
    "y = np.ravel(local[\"ArrLate\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "kf = KFold(len(X_train), n_folds=4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9be1bee504bc63c8d59d7d67782e1621",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Train a decision tree model\n",
    "\n",
    "Now that we have standardized the test sets, we are ready to apply the decision tree classifier.\n",
    "\n",
    "\n",
    "- Write a function named `fit_dt_and_predict()` that fits a decision forest classifier using [DecisionTreeClassifier()](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) in scikit learn.\n",
    "\n",
    "- Note that the function takes six arguments. **You must use `max_features` and `random_state`**, but it is not necessary that you use all of the other four arguments (`X_train`, `X_test`, `y_train`, and `y_test`). You should decide which arguments are needed and which are not.\n",
    "\n",
    "- If you read the [sklearn.tree.DecisionTreeClassifer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), there are many optional parameters that you can use in `DecisionTreeClassifier()`, e.g., `criterion`, `splitter`, etc. We will use the `max_features` and `random_state` parameters in this notebook. Use the `max_features` variable in the function definition in `DecisionTreeClassifier()`. Use defaults values for all optional parameters except `max_features` and `random_state`.\n",
    "\n",
    "- `max_features` is the number of features that are considered for each split. In my experience, this is the most important parameter. The rule of thumb that gets thrown around a lot is the square root of total number of features, but it's best to fine tune this parameter.\n",
    "\n",
    "- Without the `random_state` parameter, the algorithm has a random element. If you provide an integer to the `random_state` paramter, the algorithm becomes determinitstic and reproducible. So, for example, your code will look something like\n",
    "```python\n",
    "def fit_dt_and_predict(X_train, X_test, y_train, y_test, max_features, random_state):\n",
    "    # YOUR CODE HERE\n",
    "    DecisionTreeClassifier(\n",
    "        # YOUR CODE HERE\n",
    "        ,random_state=random_state\n",
    "    )\n",
    "    # YOUR CODE HERE\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "17ef82153f915284277e0f28d9296fd8",
     "grade": false,
     "grade_id": "fit_dt_and_predict_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def fit_dt_and_predict(X_train, X_test, y_train, y_test, max_features, random_state):\n",
    "    \"\"\"\n",
    "    Fits a SVM classifier on the training set.\n",
    "    Returns the predicted values on the test set.\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    X_train: A pandas data frame. The features of the training set.\n",
    "    X_test: A pandas data frame. The features of the test set.\n",
    "    y_train: A numpy array. The labels of the training set.\n",
    "    y_test: A numpy array. The labels of the test set.\n",
    "    max_features: An int. The number of features to consider when looking for the best split.\n",
    "    random_state: An int. The seed used by the random number generator.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A 1-D numpy array. \n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "16f22e58f190ce47e850c2aa33f502b9",
     "grade": false,
     "grade_id": "fit_dt_and_predict_run_1",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "for max_features in [1, 2, 3]:\n",
    "    scores = []\n",
    "    for train, valid in kf:\n",
    "        y_pred = fit_dt_and_predict(\n",
    "            X_train.iloc[train], X_train.iloc[valid], y_train[train], y_train[valid],\n",
    "            max_features=max_features, random_state=0\n",
    "        )\n",
    "        scores.append(accuracy_score(y_train[valid], y_pred))\n",
    "    print(\"When max_features is {0}, the mean accuracy score is {1:3.1f} %.\".format(max_features, 100.0 * np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8e88d1e0770a83bcdc00cd5b69e9f8aa",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Using only 1 feature when looking for the best split yields the greatest accuracy, so we choose `max_features=1` as the optimal model for our data set. Now that we have decided on our model, we can now train on the entire training set, and then use the test set to evaulate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "abf9b4e134bb56b80bc11646a03fa6e1",
     "grade": false,
     "grade_id": "fit_dt_and_predict_2",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred_final = fit_dt_and_predict(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    max_features=1, random_state=0\n",
    ")\n",
    "accuracy_final = accuracy_score(y_test, y_pred_final)\n",
    "print(\"test set accuracy = {0:3.1f} %\".format(100.0 * accuracy_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f74075d6276526546ea218683485a8a4",
     "grade": true,
     "grade_id": "fit_dt_and_predict_test",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(y_pred_final, np.ndarray)\n",
    "assert_equal(len(y_pred_final), len(y_test))\n",
    "assert_array_equal(\n",
    "    np.where(y_pred_final != y_test)[0],\n",
    "    [  5,   6,  12,  24,  26,  29,  31,  38,  41,  46,  49,  61,  64,\n",
    "       67,  68,  72,  83,  98, 110, 111, 125, 128, 131, 141, 142, 149,\n",
    "       156, 162, 167, 168, 186, 196, 206, 208, 213, 216, 219, 222, 228,\n",
    "       229, 236, 242, 250, 251, 252, 257, 261, 280, 295, 297, 304, 308,\n",
    "       317, 322, 327]\n",
    ")\n",
    "assert_almost_equal(accuracy_final, 0.83775811209439532)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

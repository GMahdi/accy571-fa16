{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a98c739c45025e75b7a8a39168517825",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do **not** write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select _File_ â†’ _Save and CheckPoint_)\n",
    "\n",
    "5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "e1ec759ab798ac08cdf0e87520c5ca62",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem 3. Support Vector Machine.\n",
    "\n",
    "In this problem, we fit a Support Vector Machine (SVM) model that takes the day of the week and depature delays as input and predicts whether a flight is on time or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "b478a7a7526dfa6bdaced7204200cc86",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nose.tools import assert_is_instance, assert_equal, assert_almost_equal\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal\n",
    "from pandas.util.testing import assert_index_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d83118cd03cf56b1524ddc4785042cfd",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We use the same [airline on-time performance data](http://stat-computing.org/dataexpo/2009/) from the lessons. You can find the descriptions [here](http://stat-computing.org/dataexpo/2009/). We use 4 columns: `DayOfWeek`, `ArrDelay` `DepDelay`, and `Origin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "27c6d793725982583f90c23787cc463c",
     "grade": false,
     "grade_id": "read_csv",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "filename = \"/home/data_scientist/data/2001.csv\"\n",
    "usecols = (3, 14, 15, 17)\n",
    "names = [\"DayOfWeek\", \"ArrDelay\", \"DepDelay\", \"Origin\"]\n",
    "\n",
    "all_data = pd.read_csv(filename, header=0, na_values=[\"NA\"], usecols=usecols, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0cbc9d8ab73ab60f667a62499abca433",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We perform some data pre-processing, similarly to the [Introduction to Logistic Regression](https://github.com/UI-DataScience/accy571-fa16/blob/master/Week6/notebooks/intro2lr.ipynb) notebook.\n",
    "\n",
    "To simplify the computations, we first extract only those flights that depart from Willard airport (CMI). After this, we drop all rows that have missing values (\"`NA`\") in any of the columns.\n",
    "\n",
    "We next create a categorical column, _arrival late_, that is zero if the flight arrived less than 5 minutes after the scheduled arrival time, or one if it arrived more than this number of minutes after the scheduled time. We will use this\n",
    "to train our logistic regressor.\n",
    "\n",
    "Finally, to save memory, we drop the columns that we no longer need: the origin airport and arrival delay columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3c9ade1c690fc86d370592890bde9cc4",
     "grade": false,
     "grade_id": "local",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "local = all_data[all_data[\"Origin\"] == \"CMI\"].dropna()\n",
    "\n",
    "local[\"ArrLate\"] = (local[\"ArrDelay\"] > 5).astype(int)\n",
    "\n",
    "local = local.drop([\"Origin\", \"ArrDelay\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "11828da9be7299c1c9432e6fd0242db3",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Let's print out the first 10 columns of the resulting data frame.\n",
    "\n",
    "```python\n",
    ">>> print(local.head(10))\n",
    "```\n",
    "```\n",
    "        DayOfWeek  DepDelay  ArrLate\n",
    "365879          1      15.0        1\n",
    "365880          2      -5.0        1\n",
    "365881          3      52.0        1\n",
    "365882          4      12.0        0\n",
    "365883          5       0.0        0\n",
    "365884          7     152.0        1\n",
    "365885          1      51.0        1\n",
    "365886          2       3.0        0\n",
    "365887          3      -7.0        0\n",
    "365888          4      14.0        0\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "10bc0bf383d16b5dfb942bbf5539daa3",
     "grade": false,
     "grade_id": "print_local",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(local.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7eb1d2646bc5e6668a5e9df6e47a54bc",
     "grade": false,
     "grade_id": "markdown_4",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In the previous problems, we split our data into a _training_ sample, and a testing sample by using the `train_test_split` method in scikit learn. In this notebook, we will use a [validation set](https://en.wikipedia.org/wiki/Test_set#Validation_set) in addition to the training and test sets.\n",
    "\n",
    "We split `local` into a training set (used for training our model), a validation set (used for determining hyperparameters), and a test set (used for evaluating our model's performance). We can do this by using the `train_test_split()` function twice with different `test_size` to split `local` into training:validation:test = 60:20:20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "76824f391fa05d4c82ac447b05988f6a",
     "grade": false,
     "grade_id": "train_test_split",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model by splitting into train and test sets\n",
    "X = local.drop(\"ArrLate\", axis=1)\n",
    "y = np.ravel(local[\"ArrLate\"])\n",
    "\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d7822052e211aa37e98fb0cac6f51c4e",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In [Problem 2](https://github.com/UI-DataScience/accy571-fa16/blob/master/Week6/assignments/Problem_2_Nearest_Neighbors), we saw that the columns we want to use for training have different scales, so we scaled each column to the [0, 1] range. For SVM, we will use a different scheme, and scale features to be in [-1, -1] range by dividing through the largest maximum value in each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "c9d127415d583f2587022f35a866aeb4",
     "grade": false,
     "grade_id": "standardize_define",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def standardize(df):\n",
    "    \n",
    "    result = df.apply(lambda x: x / np.max(np.abs(x)))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "fcc8173e82638e9d34534205d2d71fa9",
     "grade": false,
     "grade_id": "standardize_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X_train_standardized = standardize(X_train)\n",
    "X_valid_standardized = standardize(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "aa69056aba2cb3fbadfe3092f5bf0a39",
     "grade": false,
     "grade_id": "print_min",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(X_train_standardized.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f0db2e87e10062a9a1f32e2025b7c746",
     "grade": false,
     "grade_id": "print_max",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(X_train_standardized.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5b75fdfbf5d3d77681f36201cd09ee34",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Train a Support Vector Machine model\n",
    "\n",
    "Now that we have standardized the test sets, we are finally ready to apply the $k$-Nearest Neighbors algorithm.\n",
    "\n",
    "- Write a function named `fit_knn_and_predict()` that fits a SVM classifier using [KNeighborsClassifier()](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) in scikit learn.\n",
    "- Note that the function takes 5 arguments. You must use `kernel` but it is not necessary that you use all of the other 4 arguments (`X_train`, `X_test`, `y_train`, and `y_test`). You should decide which arguments are needed and which are not.\n",
    "- If you read the [sklearn.svm.SVC documentation](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), there are many optional parameters that you can use in `SVC()`, e.g., `C`, `kernel`, `gamma` etc. We will only use the `kernel` parameter in this notebook. Use the `kernel` variable in the function definition in `SVC()`. Use defaults values for all optional parameters except `kernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "16dbbfea631cf264b28ad7cf0446dd36",
     "grade": false,
     "grade_id": "fit_svm_and_predict_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def fit_svm_and_predict(X_train, X_test, y_train, y_test, kernel):\n",
    "    \"\"\"\n",
    "    Fits a SVM classifier on the training set.\n",
    "    Returns the predicted values on the test set.\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    X_train: A pandas data frame. The features of the training set.\n",
    "    X_test: A pandas data frame. The features of the test set.\n",
    "    y_train: A numpy array. The labels of the training set.\n",
    "    y_test: A numpy array. The labels of the test set.\n",
    "    kernel: A string. Specifies the kernel type to be used in the algorithm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A 1-D numpy array. \n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "da83f006e4619c4041d767fb3b645ca9",
     "grade": false,
     "grade_id": "fit_svm_and_predict_run_1",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred_linear = fit_svm_and_predict(\n",
    "    X_train_standardized, X_valid_standardized, y_train, y_valid,\n",
    "    kernel=\"linear\"\n",
    ")\n",
    "print(\"linear kernel accuracy = {0:3.1f} %\".format(100.0 * accuracy_score(y_valid, y_pred_linear)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "e6e7c0c5ebbff174a1169b3d7c080c87",
     "grade": false,
     "grade_id": "fit_svm_and_predict_run_2",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred_rbf = fit_svm_and_predict(\n",
    "    X_train_standardized, X_valid_standardized, y_train, y_valid,\n",
    "    kernel=\"rbf\"\n",
    ")\n",
    "print(\"rbf kernel accuracy = {0:3.1f} %\".format(100.0 * accuracy_score(y_valid, y_pred_rbf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "50d46d20d5a7f0c799ce21e4bd476740",
     "grade": false,
     "grade_id": "fit_svm_and_predict_run_3",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred_poly = fit_svm_and_predict(\n",
    "    X_train_standardized, X_valid_standardized, y_train, y_valid,\n",
    "    kernel=\"poly\"\n",
    ")\n",
    "print(\"poly kernel accuracy = {0:3.1f} %\".format(100.0 * accuracy_score(y_valid, y_pred_poly)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "71aaf7dfc4b4572dad8939db45ac2a16",
     "grade": false,
     "grade_id": "markdown_7",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The linear kernel yields the greatest accuracy, so we choose `kernel=\"linear\"` as the optimal model for our data set. Now that we have decided on our model, we can now use both the training set and the validation set for training, and then use the test set to evaulate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "0b490bed61b971390877b3d0f58a3961",
     "grade": false,
     "grade_id": "fit_svm_and_predict_run_4",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X_train_valid_standardized = standardize(X_train_valid)\n",
    "X_test_standardized = standardize(X_test)\n",
    "\n",
    "y_pred_final = fit_svm_and_predict(\n",
    "    X_train_valid_standardized, X_test_standardized, y_train_valid, y_test,\n",
    "    kernel=\"linear\"\n",
    ")\n",
    "\n",
    "accuracy_final = accuracy_score(y_test, y_pred_final)\n",
    "print(\"test set accuracy = {0:3.1f} %\".format(100.0 * accuracy_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f07b783917d1e84ec19f0186a0649f76",
     "grade": true,
     "grade_id": "fit_svm_and_predict_test",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(y_pred_final, np.ndarray)\n",
    "assert_equal(len(y_pred_final), len(y_test))\n",
    "assert_array_equal(\n",
    "    np.where(y_pred_final != y_test)[0],\n",
    "    [  5,   6,  12,  21,  24,  25,  29,  31,  35,  37,  38,  39,  41,\n",
    "       46,  48,  61,  64,  83,  86,  99, 101, 108, 110, 114, 117, 128,\n",
    "       142, 143, 159, 162, 167, 168, 170, 172, 181, 184, 189, 194, 196,\n",
    "       207, 208, 213, 219, 221, 229, 233, 236, 237, 239, 242, 246, 247,\n",
    "       250, 251, 252, 254, 269, 280, 297, 304, 312, 317, 319, 322, 328,\n",
    "       344, 345, 349, 362, 376, 383, 387, 392, 400, 405, 409, 410, 412,\n",
    "       415, 417]\n",
    ")\n",
    "assert_almost_equal(accuracy_final, 0.81132075471698117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
